[
  {
    "objectID": "syllabus/syllabus-overview.html",
    "href": "syllabus/syllabus-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "What the course catalog says: Students apply statistical analysis skills to in-depth data analysis projects ranging across diverse application areas including but not limited to energy, environmental sustainability, global health, information and culture, brain sciences, and social networks. Students practice cutting-edge statistical methods and communicate their results both technically and non-technically via presentations and written reports.\nWhat JZ says: In a few months you will enter the working world as a professional with a degree in statistics. When you’re out there, a few things may happen:\n\nyou will be thrown onto a team with people you didn’t choose;\nyou will be told to use software you haven’t necessarily used;\nyou will be asked to study data from an unfamiliar domain;\nyou will be required to use new statistical methods you haven’t previously studied;\nyou will have to communicate your work to managers and stakeholders that know nothing about statistics.\n\nAre you ready for all that? To prepare you for the realities of modern data science practice, this class is essentially a sequence of group projects (case studies) where we confront you with messy, real world data analysis problems and coach you through how to tackle them using the statistical methods and the productivity tools (Quarto, R, Git, etc) that you studied in previous classes. Along the way, the instructors will introduce new statistical topics that you may find useful.\nPrerequisites: STA 360 or 402 (i.e. Bayes)."
  },
  {
    "objectID": "syllabus/syllabus-overview.html#description",
    "href": "syllabus/syllabus-overview.html#description",
    "title": "Course overview",
    "section": "",
    "text": "What the course catalog says: Students apply statistical analysis skills to in-depth data analysis projects ranging across diverse application areas including but not limited to energy, environmental sustainability, global health, information and culture, brain sciences, and social networks. Students practice cutting-edge statistical methods and communicate their results both technically and non-technically via presentations and written reports.\nWhat JZ says: In a few months you will enter the working world as a professional with a degree in statistics. When you’re out there, a few things may happen:\n\nyou will be thrown onto a team with people you didn’t choose;\nyou will be told to use software you haven’t necessarily used;\nyou will be asked to study data from an unfamiliar domain;\nyou will be required to use new statistical methods you haven’t previously studied;\nyou will have to communicate your work to managers and stakeholders that know nothing about statistics.\n\nAre you ready for all that? To prepare you for the realities of modern data science practice, this class is essentially a sequence of group projects (case studies) where we confront you with messy, real world data analysis problems and coach you through how to tackle them using the statistical methods and the productivity tools (Quarto, R, Git, etc) that you studied in previous classes. Along the way, the instructors will introduce new statistical topics that you may find useful.\nPrerequisites: STA 360 or 402 (i.e. Bayes)."
  },
  {
    "objectID": "syllabus/syllabus-overview.html#meetings",
    "href": "syllabus/syllabus-overview.html#meetings",
    "title": "Course overview",
    "section": "Meetings",
    "text": "Meetings\n\n\n\n\n\n\n\n\n\nMeeting\nLocation\nTime\nStaff\n\n\n\n\nLecture 001\nPerkins LINK 071 (Classroom 5)\nTuTh 8:30 AM - 9:45 AM\nJohn\n\n\nLecture 002\nPerkins LINK 071 (Classroom 5)\nMoWe 10:05 AM - 11:20 AM\nEd\n\n\nLab 01\nOld Chemistry 001\nF 1:25 PM - 2:40 PM\nAidan\n\n\nLab 02\nPerkins LINK 071 (Classroom 5)\nF 10:05 AM - 11:20 AM\nLuigi\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou must attend the lecture and lab section that you are enrolled in. Class activities will often be completed with your project groups, and project groups will be randomly assigned within sections."
  },
  {
    "objectID": "syllabus/syllabus-overview.html#team",
    "href": "syllabus/syllabus-overview.html#team",
    "title": "Course overview",
    "section": "Team",
    "text": "Team\n\n\n\n\n\n\n\n\n\nMug\nName\nRole\nOffice Hours\n\n\n\n\n\nIversen, Ed\nInstructor\nTh 4:00 PM - 5:00 PM\nOld Chemistry 122B\n\n\n\nZito, John\nInstructor\nTBD\nOld Chemistry 207\n\n\n\nKnox, Mary\nCourse Coordinator\nnone\n\n\n\nFan, Li\nTA\nM 7:00 PM - 9:00 PM\nZoom (link on Canvas)\n\n\n\nGleich, Aidan\nLab TA\nnone\n\n\n\nLi, Aihua\nTA\nW 2:30 PM - 4:30 PM\nOld Chem 203B\n\n\n\nMalgieri, Luigi\nLab TA\nnone\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe four TAs work with both sections, so feel free to seek help from any of them."
  },
  {
    "objectID": "syllabus/syllabus-resources.html",
    "href": "syllabus/syllabus-resources.html",
    "title": "University resources",
    "section": "",
    "text": "If you are having difficulty with the costs associated with this course (obtaining a laptop, mostly), here are some resources:\n\nKarsh Office of Undergraduate Support: Regardless of your aid package, Karsh offers loans and resources for connecting students with campus programs that might help alleviate course costs.\nDukeLIFE: The Course Material Assistance program offers assistance for eligible students, including through the LIFE Loaner Laptop Program. Students who are eligible for DukeLIFE benefits are notified before the start of the semester; program resources are limited.\nDuke Link: They have a small supply of laptops that can be rented out for five days at a time."
  },
  {
    "objectID": "syllabus/syllabus-resources.html#course-costs",
    "href": "syllabus/syllabus-resources.html#course-costs",
    "title": "University resources",
    "section": "",
    "text": "If you are having difficulty with the costs associated with this course (obtaining a laptop, mostly), here are some resources:\n\nKarsh Office of Undergraduate Support: Regardless of your aid package, Karsh offers loans and resources for connecting students with campus programs that might help alleviate course costs.\nDukeLIFE: The Course Material Assistance program offers assistance for eligible students, including through the LIFE Loaner Laptop Program. Students who are eligible for DukeLIFE benefits are notified before the start of the semester; program resources are limited.\nDuke Link: They have a small supply of laptops that can be rented out for five days at a time."
  },
  {
    "objectID": "syllabus/syllabus-resources.html#tech-support",
    "href": "syllabus/syllabus-resources.html#tech-support",
    "title": "University resources",
    "section": "Tech support",
    "text": "Tech support\nContact the Duke OIT Service Desk at oit.duke.edu/help."
  },
  {
    "objectID": "syllabus/syllabus-resources.html#writing-studio",
    "href": "syllabus/syllabus-resources.html#writing-studio",
    "title": "University resources",
    "section": "Writing Studio",
    "text": "Writing Studio\nPlease feel encouraged to set up a synchronous online appointment with the Writing Studio, a place beyond our classroom to work collaboratively with an attentive, nonevaluative reader. You can schedule an appointment at any stage in your writing process, including before you have even started writing. You’ll find friendly student consultants who are eager to talk with you about your writing and think with you about ways to make your processes even more effective. Visit https://twp.duke.edu/twp-writing-studio to schedule an appointment and to learn more about Writing Studio resources."
  },
  {
    "objectID": "syllabus/syllabus-resources.html#academic-support",
    "href": "syllabus/syllabus-resources.html#academic-support",
    "title": "University resources",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (the ARC) offers services to support students academically during their undergraduate careers at Duke. The ARC can provide support with time management, academic skills and strategies, course-specific tutoring, and more. ARC services are available free to all Duke undergraduate student studying any discipline.\nYou can contact the Academic Resource Center by phone at (919) 684-5917, by email at theARC@duke.edu, or by visiting http://arc.duke.edu/."
  },
  {
    "objectID": "syllabus/syllabus-resources.html#accessibility",
    "href": "syllabus/syllabus-resources.html#accessibility",
    "title": "University resources",
    "section": "Accessibility",
    "text": "Accessibility\nIf any portion of the course is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students can engage with their courses and related assignments. Students should contact the SDAO to request or update accommodations under these circumstances."
  },
  {
    "objectID": "syllabus/syllabus-resources.html#mental-health-and-well-being",
    "href": "syllabus/syllabus-resources.html#mental-health-and-well-being",
    "title": "University resources",
    "section": "Mental health and well-being",
    "text": "Mental health and well-being\nDuke is committed to holistic student wellbeing, which includes one’s mental, emotional, and physical health. The university offers resources to help students manage daily stress, to encourage intentional self-care, and to access just-in-time support. If you find you need support, your mental and/or emotional health concerns are impacting your day-to-day activities, your academic performance, or you need someone to talk to, the resources below are available to you:\n\nDukeReach: DukeReach provides comprehensive outreach services to support students in managing all aspects of wellbeing, including referrals and follow-up services for students who are experiencing significant challenges related to mental health, physical health, social adjustment, and/or a variety of other stressors. You can contact the DukeReach team at dukereach@duke.edu;\nCounseling and Psychological Services (CAPS): CAPS offers counseling services to Duke students including virtual appointments, and referrals in the community. You do not need an appointment for an initial assessment. You may walk in or call 919-660-1000 to get started. Hours: Monday-Friday 9:00am - 4:00pm. After hours counseling services are available at no additional cost to students, you can call: 919-660-1000 Option 2;\nTimelyCare: TimelyCare is an online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling;\nDuke Student Health: Student Health offers a wide range of healthcare services for all Duke students, many of which are covered by the student health fee. To make an appointment call (919) 681-9355. Hours: Monday - Friday, 8am - 4:30pm, Thursday 9am - 4:30pm. Closed from 12-12:30 each day."
  },
  {
    "objectID": "slides/case-study-2.html#a-few-us-macro-aggregates",
    "href": "slides/case-study-2.html#a-few-us-macro-aggregates",
    "title": "Case Study 2",
    "section": "A few US macro aggregates",
    "text": "A few US macro aggregates\n\n\n\n\n\n\n\n\nThere are tons more. Play around on FRED!"
  },
  {
    "objectID": "slides/case-study-2.html#where-do-these-data-come-from",
    "href": "slides/case-study-2.html#where-do-these-data-come-from",
    "title": "Case Study 2",
    "section": "Where do these data come from?",
    "text": "Where do these data come from?\nBureau of Labor Statistics (under the Labor Department)\n\n\nConsumer Price Index (CPI);\n\n\nUnemployment;\n\nLabor Productivity;\n\nBureau of Economic Analysis (under the Commerce Department)\n\n\nGross Domestic Product (GDP);\n\nPersonal Consumption Expenditures (PCE).\n\nThere are 13 principal statistical agencies in the US federal government."
  },
  {
    "objectID": "slides/case-study-2.html#who-cares",
    "href": "slides/case-study-2.html#who-cares",
    "title": "Case Study 2",
    "section": "Who cares?",
    "text": "Who cares?\n\nThese are some of the most talked about data in the world. They are constantly being studied by…\n\n\n\n\n\n\n\n\n\nAcademics\n“how does the macroeconomy…work?”\n\n\nPolicymakers\n“what effect did our actions have?”\n\n\nBusinesses\n“how do we plan for the future?”\n\n\nJournalists\neach new release is a major headline…\n\n\nInvestors\n…followed by a second headline about how the stock and bond markets reacted.\n\n\n\n\n\nDid I leave anybody out?"
  },
  {
    "objectID": "slides/case-study-2.html#oh-right.",
    "href": "slides/case-study-2.html#oh-right.",
    "title": "Case Study 2",
    "section": "Oh, right.",
    "text": "Oh, right."
  },
  {
    "objectID": "slides/case-study-2.html#what-happened",
    "href": "slides/case-study-2.html#what-happened",
    "title": "Case Study 2",
    "section": "What happened?",
    "text": "What happened?\n\nAt 8:30 AM ET on Friday August 1, 2025, the BLS issued its regular monthly report on the US employment situation;\n\nIt includes total nonfarm payroll employment:\n\na measure of the number of U.S. workers in the economy that excludes proprietors, private household employees, unpaid volunteers, farm employees, and the unincorporated self-employed. This measure accounts for approximately 80 percent of the workers who contribute to Gross Domestic Product (GDP). This measure provides useful insights into the current economic situation because it can represent the number of jobs added or lost in an economy.\n\n\nPresidents want to take credit for this going up each month."
  },
  {
    "objectID": "slides/case-study-2.html#what-happened-1",
    "href": "slides/case-study-2.html#what-happened-1",
    "title": "Case Study 2",
    "section": "What happened?",
    "text": "What happened?\nThe August 1 report announced the initial release of the July numbers, as well as revisions to the May and June numbers:\n\n\nSource: Madeleine Ngo in the New York Times, August 1 2025."
  },
  {
    "objectID": "slides/case-study-2.html#what-happened-2",
    "href": "slides/case-study-2.html#what-happened-2",
    "title": "Case Study 2",
    "section": "What happened?",
    "text": "What happened?\nThe Commissioner of Labor Statistics was fired that afternoon:\n\n\n\n\nSource: Truth Social.\n\n\n\n\nSource: Truth Social"
  },
  {
    "objectID": "slides/case-study-2.html#whence-revisions",
    "href": "slides/case-study-2.html#whence-revisions",
    "title": "Case Study 2",
    "section": "Whence revisions?",
    "text": "Whence revisions?\nThe statistical agencies announce an initial estimate with a one period lag, but they continue to revise the measurement (sometimes years later) as new information arrives and measurement techniques improve.\n\nFrom the BLS report (and more here):\n\nMonthly revisions result from additional reports received from businesses and government agencies since the last published estimates and from the recalculation of seasonal factors.\n\n\n\nFrom Ben Casselman at the New York Times:\n\nThe monthly numbers are based on a huge survey of businesses and other employers. Not all businesses respond in time for the initial estimate, however, forcing government statisticians to fill in the gaps with a statistical technique that essentially assumes the businesses that didn’t respond behaved the same way as the ones that did. That approach works fine during normal times. But during periods of rapid change, that assumption can be misleading."
  },
  {
    "objectID": "slides/case-study-2.html#frbps-real-time-data-set",
    "href": "slides/case-study-2.html#frbps-real-time-data-set",
    "title": "Case Study 2",
    "section": "FRBP’s Real-Time Data Set",
    "text": "FRBP’s Real-Time Data Set\n\n\n\n\n\nTracks the entire history of revisions for key macroeconomic variables.\n\nCheck it out!"
  },
  {
    "objectID": "slides/case-study-2.html#what-are-these-data-like",
    "href": "slides/case-study-2.html#what-are-these-data-like",
    "title": "Case Study 2",
    "section": "What are these data like?",
    "text": "What are these data like?\nQuarterly GDP growth (year-over-year percent change):\n\n\n# A tibble: 244 × 10\n   DATE    v65Q4 v66Q1 v66Q2 v66Q3 v66Q4 v67Q1 v67Q2 v67Q3 v67Q4\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1965:Q1  5.36  5.40  5.40  5.37  5.37  5.37  5.37  5.32  5.32\n 2 1965:Q2  4.43  4.79  4.79  5.14  5.14  5.14  5.14  5.38  5.38\n 3 1965:Q3  4.55  5.22  5.22  5.68  5.68  5.68  5.68  5.96  5.96\n 4 1965:Q4 NA     6.33  6.79  7.49  7.49  7.49  7.49  7.80  7.80\n 5 1966:Q1 NA    NA     6.04  6.70  6.70  6.70  6.70  7.30  7.30\n 6 1966:Q2 NA    NA    NA     5.99  5.87  5.87  5.87  6.49  6.49\n 7 1966:Q3 NA    NA    NA    NA     5.26  5.13  5.13  5.49  5.49\n 8 1966:Q4 NA    NA    NA    NA    NA     4.09  4.12  4.21  4.21\n 9 1967:Q1 NA    NA    NA    NA    NA    NA     2.61  2.37  2.37\n10 1967:Q2 NA    NA    NA    NA    NA    NA    NA     2.36  2.37\n# ℹ 234 more rows\n\n\n\nA single variable gets a whole data frame with an upper triangular structure:\n\nrow = what period are we measuring?\ncolumn = when were we measuring it?"
  },
  {
    "objectID": "slides/case-study-2.html#what-are-these-data-like-1",
    "href": "slides/case-study-2.html#what-are-these-data-like-1",
    "title": "Case Study 2",
    "section": "What are these data like?",
    "text": "What are these data like?\nQuarterly GDP growth (year-over-year percent change):\n\noutput_growth |&gt;\n  select(DATE, v66Q3) |&gt;\n  filter(DATE == \"1965:Q3\")\n\n# A tibble: 1 × 2\n  DATE    v66Q3\n  &lt;chr&gt;   &lt;dbl&gt;\n1 1965:Q3  5.68\n\n\nThis is our estimate of GDP growth in 1965:Q3 as measured a year later in 1966:Q3."
  },
  {
    "objectID": "slides/case-study-2.html#columns-are-vintages",
    "href": "slides/case-study-2.html#columns-are-vintages",
    "title": "Case Study 2",
    "section": "Columns are vintages\n",
    "text": "Columns are vintages\n\nQuarterly GDP growth (year-over-year percent change):\n\noutput_growth |&gt;\n  select(DATE, v66Q3)\n\n# A tibble: 304 × 2\n   DATE    v66Q3\n   &lt;chr&gt;   &lt;dbl&gt;\n 1 1950:Q1  4.65\n 2 1950:Q2  8.06\n 3 1950:Q3 11.3 \n 4 1950:Q4 14.5 \n 5 1951:Q1 10.4 \n 6 1951:Q2  9.47\n 7 1951:Q3  7.14\n 8 1951:Q4  5.03\n 9 1952:Q1  4.43\n10 1952:Q2  2.12\n# ℹ 294 more rows\n\n\nThis column is the 1966:Q3 vintage of the data. This is history as it was understood in 1966:Q3."
  },
  {
    "objectID": "slides/case-study-2.html#rows-are-the-history-of-revisions",
    "href": "slides/case-study-2.html#rows-are-the-history-of-revisions",
    "title": "Case Study 2",
    "section": "Rows are the history of revisions",
    "text": "Rows are the history of revisions\nQuarterly GDP growth (year-over-year percent change):\n\noutput_growth |&gt;\n  filter(DATE == \"1965:Q3\")\n\n\n\n# A tibble: 1 × 11\n  DATE    v65Q4 v66Q1 v66Q2 v66Q3 v66Q4 v67Q1 v67Q2 v67Q3 v67Q4 v68Q1\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1965:Q3  4.55  5.22  5.22  5.68  5.68  5.68  5.68  5.96  5.96  5.96\n\n\nHow did our measurement for 1965:Q3 evolve over time as it was revised?"
  },
  {
    "objectID": "slides/case-study-2.html#in-my-life",
    "href": "slides/case-study-2.html#in-my-life",
    "title": "Case Study 2",
    "section": "In My Life",
    "text": "In My Life\n\n\n\n\n\nMoving across the columns:"
  },
  {
    "objectID": "slides/case-study-2.html#the-path-of-revisions",
    "href": "slides/case-study-2.html#the-path-of-revisions",
    "title": "Case Study 2",
    "section": "The path of revisions",
    "text": "The path of revisions\nMoving across a fixed row:"
  },
  {
    "objectID": "slides/case-study-2.html#how-do-data-revisions-behave",
    "href": "slides/case-study-2.html#how-do-data-revisions-behave",
    "title": "Case Study 2",
    "section": "How do data revisions behave?",
    "text": "How do data revisions behave?\n\n\n\n\n\n\n\nGood heavens"
  },
  {
    "objectID": "slides/case-study-2.html#how-do-data-revisions-behave-1",
    "href": "slides/case-study-2.html#how-do-data-revisions-behave-1",
    "title": "Case Study 2",
    "section": "How do data revisions behave?",
    "text": "How do data revisions behave?\nAruoba (2008 JMCB):\n\nWe document the empirical properties of revisions to major macroeconomic variables in the United States. Our findings suggest that they do not satisfy simple desirable statistical properties. In particular, we find that these revisions do not have a zero mean, which indicates that the initial announcements by statistical agencies are biased. We also find that the revisions are quite large compared to the original variables and they are predictable using the information set at the time of the initial announcement, which means that the initial announcements of statistical agencies are not rational forecasts.\n\n\nBummer."
  },
  {
    "objectID": "slides/case-study-2.html#this-is-probably-getting-worse",
    "href": "slides/case-study-2.html#this-is-probably-getting-worse",
    "title": "Case Study 2",
    "section": "This is probably getting worse",
    "text": "This is probably getting worse\n\nFederal statistical agencies have faced mounting challenges in recent years as Americans have become more reluctant to respond to the surveys that are the basis for much of the nation’s economic data. Shrinking budgets have made it harder to make up for falling response rates, and to develop new approaches to replace surveys altogether.\n\nSource: New York Times."
  },
  {
    "objectID": "slides/case-study-2.html#picture-this",
    "href": "slides/case-study-2.html#picture-this",
    "title": "Case Study 2",
    "section": "Picture this",
    "text": "Picture this\nImagine you work at the Federal Reserve, or the Congressional Budget Office, or Goldman Sachs, or The Wall Street Journal. Your boss taps you on the shoulder and says:\n\nWe just got an unexpected data release, but we know it will be revised. And in general, I’m sick of this crap where the numbers swing around for months after the fact and we don’t know where we stand. Can you develop a model that can predict where the measurement will settle after the revisions are done?"
  },
  {
    "objectID": "slides/case-study-2.html#your-task",
    "href": "slides/case-study-2.html#your-task",
    "title": "Case Study 2",
    "section": "Your task",
    "text": "Your task\nData: the full set of historical vintages for eight macro variables.\nEach team will be assigned a target variable. Then:\n\nDevelop a model that can predict the final release of the variable using only information available at the time of the initial release;\n\ntreat the vintage three years later as the final release;\nUse the other variables as predictors if you want;\nProduce a method with good historical performance averaged over time;\n\n\nYou must quantify uncertainty. Point predictions are not enough. You need to produce and evaluate full predictive distributions that incorporate as many sources of uncertainty as possible."
  },
  {
    "objectID": "slides/case-study-2.html#your-task-1",
    "href": "slides/case-study-2.html#your-task-1",
    "title": "Case Study 2",
    "section": "Your task",
    "text": "Your task\nGiven the latest release and everything that’s come before, can you predict where the revisions will ultimately settle down?"
  },
  {
    "objectID": "slides/case-study-2.html#deadlines",
    "href": "slides/case-study-2.html#deadlines",
    "title": "Case Study 2",
    "section": "Deadlines",
    "text": "Deadlines\n\nEDA presentations (Wed 2/11):\n\nthe usual plots and summaries to get a feel and motivate your analysis;\nhit the books and teach the class how your assigned variable is actually measured, and what goes on during the revision process;\n\n\nAnalysis presentations (Wed 2/18):\n\nwhat models did you consider?\nhow did their predictive accuracy compare?\ncan you interpret why the models performed the way they did?\n\n\nFinal submission (Mon 2/23)."
  },
  {
    "objectID": "slides/case-study-2.html#lecture-topics",
    "href": "slides/case-study-2.html#lecture-topics",
    "title": "Case Study 2",
    "section": "Lecture topics",
    "text": "Lecture topics\nA crash course in time series:\n\nAutoregressive moving average (ARMA) models;\nDynamic linear models (DLMs);\nProbabilistic prediction;\nTime series cross-validation (“leave-future-out”).\n\nYou may not ultimately choose to use these specific models, but the evaluation techniques are model-agnostic."
  },
  {
    "objectID": "slides/case-study-2.html#words-of-caution",
    "href": "slides/case-study-2.html#words-of-caution",
    "title": "Case Study 2",
    "section": "Words of caution",
    "text": "Words of caution\n\nThe data come in this funky, unfamiliar form: each variable gets its own data frame with this triangular structure (row = period being measured; column = vintage). How are you going to deal with that?\nWhen you generate a prediction, make sure you are conditioning only on information that would have been available at that time!\nThere already exists a massive literature on this, which you are welcome to explore. However, you will quickly become overwhelmed if you’re not careful. That’s part of the challenge!\nBlack box machine learning methods may or may not work well here, but if you don’t know how to get predictive uncertainty quantification from them, or you find that the UQ is unreliable, then say goodbye to XGBoost!"
  },
  {
    "objectID": "slides/case-study-2.html#not-just-point-prediction",
    "href": "slides/case-study-2.html#not-just-point-prediction",
    "title": "Case Study 2",
    "section": "Not just point prediction",
    "text": "Not just point prediction"
  },
  {
    "objectID": "slides/case-study-2.html#point-prediction",
    "href": "slides/case-study-2.html#point-prediction",
    "title": "Case Study 2",
    "section": "Point prediction",
    "text": "Point prediction\nYour single-number best guess at tomorrow’s observation:"
  },
  {
    "objectID": "slides/case-study-2.html#prediction-interval",
    "href": "slides/case-study-2.html#prediction-interval",
    "title": "Case Study 2",
    "section": "Prediction interval",
    "text": "Prediction interval\nA range of likely values for tomorrow’s observation:"
  },
  {
    "objectID": "slides/case-study-2.html#prediction-distribution-density",
    "href": "slides/case-study-2.html#prediction-distribution-density",
    "title": "Case Study 2",
    "section": "Prediction distribution (density)",
    "text": "Prediction distribution (density)\nFull distribution capturing uncertainty about tomorrow:"
  },
  {
    "objectID": "slides/case-study-2.html#and-then-tomorrow-finally-comes",
    "href": "slides/case-study-2.html#and-then-tomorrow-finally-comes",
    "title": "Case Study 2",
    "section": "And then tomorrow finally comes",
    "text": "And then tomorrow finally comes\nSo…how’d we do? Any ideas?"
  },
  {
    "objectID": "slides/case-study-2.html#whats-the-point",
    "href": "slides/case-study-2.html#whats-the-point",
    "title": "Case Study 2",
    "section": "What’s the point?",
    "text": "What’s the point?\n\nWe want intervals and densities to communicate uncertainty about the prediction;\n\nWhat sources of uncertainty?\n\nData uncertainty (data are realization of random process);\nParameter estimation uncertainty;\nHyperparameter tuning uncertainty;\nModel uncertainty;\nUncertainty introduced by missing data;\nWhat else?\n\n\n\n\nNewsflash: you have seen this before."
  },
  {
    "objectID": "slides/case-study-2.html#regression-101-interval-estimation",
    "href": "slides/case-study-2.html#regression-101-interval-estimation",
    "title": "Case Study 2",
    "section": "Regression 101: interval estimation",
    "text": "Regression 101: interval estimation\nRemember this picture?"
  },
  {
    "objectID": "slides/case-study-2.html#regression-101",
    "href": "slides/case-study-2.html#regression-101",
    "title": "Case Study 2",
    "section": "Regression 101",
    "text": "Regression 101\nRecall the simple linear model:\n\\[\n\\begin{aligned}\ny_i&=\\mu(x_i)+\\varepsilon_i=\\beta_0+\\beta_1x_i+\\varepsilon_i, && \\varepsilon_i\\iid\\N(0\\com\\sigma^2)\\\\\n\\end{aligned}\n\\]\n\nThe OLS estimators are:\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1\n&=\n\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{S_{xx}},&&S_{xx}=\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2\n\\\\\n\\hat{\\beta}_0\n&=\n\\bar{y}-\\bar{x}\\hat{\\beta}_1\n\\\\\n\\hat{\\mu}(x)\n&=\n\\hat{\\beta}_0+\\hat{\\beta_1}x\n\\\\\n\\widehat{\\sigma^2}\n&=\n\\frac{1}{n-2}\\sum\\limits_{i=1}^n[y_i-\\hat{\\mu}(x_i)]^2.\n\\end{aligned}\n\\]\nThe main idea of classical statistics: estimators are random variables as a function of the data. Estimation uncertainty comes from the sampling process."
  },
  {
    "objectID": "slides/case-study-2.html#sampling-distributions",
    "href": "slides/case-study-2.html#sampling-distributions",
    "title": "Case Study 2",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nYou can show that the sampling distributions are independent, and\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\hat{\\beta}_0\n\\\\\n\\hat{\\beta}_1\n\\end{bmatrix}\n&\\sim\n\\text{N}_2\\left(\\begin{bmatrix}\\beta_0\\\\\\beta_1\\end{bmatrix}\n\\com\n\\sigma^2\n\\begin{bmatrix}\n\\frac{1}{n}+\\frac{\\bar{x}^2}{S_{xx}}&-\\bar{x}/S_{xx}\\\\-\\bar{x}/S_{xx}&1/S_{xx}\n\\end{bmatrix}\\right)\n\\\\\n\\widehat{\\sigma^2}\n&\\sim\n\\text{Gamma}\\left(\\frac{n-2}{2}\\com\\frac{n-2}{2\\sigma^2}\\right)\n.\n\\end{aligned}\n\\]\n\nThe estimator of the regression function is the sum of two correlated Gaussian terms, so it stays normal, you add the means, and you add the variances, adjusting for the covariance:\n\n\n\\[\n\\hat{\\mu}(x)=\\hat{\\beta}_0+\\hat{\\beta_1}x\\sim\\N\\left(\\beta_0+\\beta_1x\\com \\sigma^2\\left[\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}\\right]\\right)\n\\]\n\n\nTake my word for it!"
  },
  {
    "objectID": "slides/case-study-2.html#the-confidence-interval-for-the-line",
    "href": "slides/case-study-2.html#the-confidence-interval-for-the-line",
    "title": "Case Study 2",
    "section": "The confidence interval for the line",
    "text": "The confidence interval for the line\nA tale of two pivots:\n\n\\[\n\\frac{\\hat{\\mu}(x)-\\mu(x)}{\\sigma\\sqrt{\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}}}\n\\sim\\N(0\\com 1) \\quad\\implies\\quad \\frac{\\hat{\\mu}(x)-\\mu(x)}{\\hat{\\sigma}\\sqrt{\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}}}\n\\sim t_{n-2}.\n\\]\n\n\nSo we can use quantiles of the \\(t\\) distribution to get an exact interval for the unknow regression function at a new \\(x\\):\n\n\n\\[\n\\hat{\\mu}(x)\\pm t^\\star_{n-2}\\times \\hat{\\sigma}\\sqrt{\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}}.\n\\]\nThis quantifies frequentist sampling uncertainty for the regression line."
  },
  {
    "objectID": "slides/case-study-2.html#the-predictive-pivot",
    "href": "slides/case-study-2.html#the-predictive-pivot",
    "title": "Case Study 2",
    "section": "The predictive pivot",
    "text": "The predictive pivot\nIf a new \\(\\tilde{x}\\) joins the party, we have\n\n\n\\(\\tilde{y}\\sim\\text{N}(\\mu(\\tilde{x})\\com\\sigma^2)\\);\n\n\\(\\hat{\\mu}(x)\\sim\\N\\left(\\mu(\\tilde{x})\\com \\sigma^2\\left[\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}\\right]\\right)\\);\nThese are independent.\n\n\nTheir difference is normal, and you subtract the means and add the variances:\n\\[\n\\begin{aligned}\n\\hat{\\mu}(\\tilde{x})-\\tilde{y} &\\sim\\N\\left(\\mu(\\tilde{x})-\\mu(\\tilde{x})\\com \\sigma^2\\left[\\frac{1}{n}+\\frac{(\\tilde{x}-\\bar{x})^2}{S_{xx}}\\right]+\\sigma^2\\right)\n\\\\\n&\\sim\\N\\left(0\\com \\sigma^2\\left[1+\\frac{1}{n}+\\frac{(\\tilde{x}-\\bar{x})^2}{S_{xx}}\\right]\\right).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/case-study-2.html#the-prediction-interval-for-a-new-observation",
    "href": "slides/case-study-2.html#the-prediction-interval-for-a-new-observation",
    "title": "Case Study 2",
    "section": "The prediction interval for a new observation",
    "text": "The prediction interval for a new observation\nA tale of two more pivots:\n\n\\[\n\\frac{\\hat{\\mu}(\\tilde{x})-\\tilde{y}}{\\sigma\\sqrt{1+\\frac{1}{n}+\\frac{(\\tilde{x}-\\bar{x})^2}{S_{xx}}}}\n\\sim\\N(0\\com 1) \\quad\\implies\\quad \\frac{\\hat{\\mu}(\\tilde{x})-\\tilde{y}}{\\hat{\\sigma}\\sqrt{1+\\frac{1}{n}+\\frac{(\\tilde{x}-\\bar{x})^2}{S_{xx}}}}\n\\sim t_{n-2}.\n\\]\n\n\nExact prediction interval for a yet-to-be-observed \\(\\tilde{y}\\):\n\n\n\\[\n\\hat{\\mu}(\\tilde{x})\\pm t^\\star_{n-2}\\times \\hat{\\sigma}\\sqrt{1+\\frac{1}{n}+\\frac{(\\tilde{x}-\\bar{x})^2}{S_{xx}}}.\n\\]\n\n\nThis is wider than the confidence interval because the inherent random sampling of the new \\(\\tilde{y}\\) (its \\(\\tilde{\\varepsilon}\\)) adds a second source of uncertainty."
  },
  {
    "objectID": "slides/case-study-2.html#comparison",
    "href": "slides/case-study-2.html#comparison",
    "title": "Case Study 2",
    "section": "Comparison",
    "text": "Comparison\n\n\nConfidence interval for \\(\\mu(\\tilde{x})\\).\n\\[\n\\hat{\\mu}(\\tilde{x})\\pm t^\\star_{n-2}\\times \\hat{\\sigma}\\sqrt{\\frac{1}{n}+\\frac{(\\tilde{x}-\\bar{x})^2}{S_{xx}}}.\n\\]\nOne source of uncertainty:\n\nestimation uncertainty from \\(\\hat{\\beta}\\).\n\n\nPrediction interval for \\(\\tilde{y}\\).\n\\[\n\\hat{\\mu}(\\tilde{x})\\pm t^\\star_{n-2}\\times \\hat{\\sigma}\\sqrt{1+\\frac{1}{n}+\\frac{(\\tilde{x}-\\bar{x})^2}{S_{xx}}}.\n\\]\nTwo sources of uncertainty:\n\nestimation uncertainty from \\(\\hat{\\beta}\\);\nnew \\(\\tilde{\\varepsilon}\\). That’s where we picked up the extra 1 in the standard error."
  },
  {
    "objectID": "slides/case-study-2.html#so-the-pi-is-wider-than-the-ci",
    "href": "slides/case-study-2.html#so-the-pi-is-wider-than-the-ci",
    "title": "Case Study 2",
    "section": "So the PI is wider than the CI",
    "text": "So the PI is wider than the CI\nBehold:\n\nmtcars_fit &lt;- lm(mpg ~ wt, data = mtcars)\nxnew &lt;- data.frame(wt = 4.5)\n\npredict(mtcars_fit, xnew, interval = \"confidence\")\n\n     fit      lwr      upr\n1 13.235 11.40347 15.06654\n\npredict(mtcars_fit, xnew, interval = \"prediction\")\n\n     fit      lwr      upr\n1 13.235 6.750452 19.71956"
  },
  {
    "objectID": "slides/case-study-2.html#hence-the-famous-picture",
    "href": "slides/case-study-2.html#hence-the-famous-picture",
    "title": "Case Study 2",
    "section": "Hence the famous picture",
    "text": "Hence the famous picture"
  },
  {
    "objectID": "slides/case-study-2.html#bayes-101-posterior-predictive-distribution",
    "href": "slides/case-study-2.html#bayes-101-posterior-predictive-distribution",
    "title": "Case Study 2",
    "section": "Bayes 101: posterior predictive distribution",
    "text": "Bayes 101: posterior predictive distribution\nThe posterior for parameters:\n\\[\np(\\boldsymbol{\\theta}\\mid y_{1:n})\n=\n\\frac{p(y_{1:n}\\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(y_{1:n})}.\n\\]\n\nThe posterior for a new observation:\n\\[\np(\\tilde{y}\\mid y_{1:n})=\\int p(\\tilde{y}\\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}\\mid y_{1:n})\\,\\text{d}\\boldsymbol{\\theta}\n.\n\\]\n\n\nSimilar to before, it incorporates parameter uncertainty from the posterior and inherent randomness of new \\(\\tilde{y}\\) from the likelihood."
  },
  {
    "objectID": "slides/case-study-2.html#example-linear-regression-again",
    "href": "slides/case-study-2.html#example-linear-regression-again",
    "title": "Case Study 2",
    "section": "Example: linear regression, again",
    "text": "Example: linear regression, again\nConsider linear regression where the prior \\(p(\\sigma^2\\com\\Bbeta)\\) is conjugate:\n\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0\\com b_0)\n\\\\\n\\Bbeta\\mid \\sigma^2\n&\\sim\n\\text{N}_{p}(\\bar{\\Bbeta}_0\\com\\sigma^2\\BH^{-1}_0)\n\\\\\ny_i\n\\mid\n\\Bx_i\n\\com\n\\Bbeta\\com\\sigma^2\n&\\iid \\text{N}\n\\left(\n\\Bx_i^\\tr\\Bbeta\\com\\sigma^2\n\\right).\n\\end{aligned}\n\\]\n\n\nWithout revisiting the pain and tedium of the calculation, the posterior \\(p(\\sigma^2\\com\\Bbeta\\mid y_{1:n}\\com \\Bx_{1:n})\\) remains in the normal-inverse-gamma family with updated hyperparameters:\n\\[\n\\begin{aligned}\n\\sigma^2\\mid y_{1:n}\\com \\Bx_{1:n}\n&\\sim\n\\text{IG}(a_n\\com b_n)\n\\\\\n\\Bbeta\\mid \\sigma^2\\com y_{1:n}\\com \\Bx_{1:n}\n&\\sim\n\\text{N}_{p}(\\bar{\\Bbeta}_n\\com\\sigma^2\\BH^{-1}_n)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/case-study-2.html#example-linear-regression-again-1",
    "href": "slides/case-study-2.html#example-linear-regression-again-1",
    "title": "Case Study 2",
    "section": "Example: linear regression, again",
    "text": "Example: linear regression, again\nImagine we’ve observed the \\((\\Bx_i\\com y_i)\\), and then a new \\(\\tilde{\\Bx}\\) joins the party:\n\n\\[\n\\begin{aligned}\n\\sigma^2\\mid y_{1:n}\\com \\Bx_{1:n}\n&\\sim\n\\text{IG}(a_n\\com b_n)\n\\\\\n\\Bbeta\\mid \\sigma^2\\com y_{1:n}\\com \\Bx_{1:n}\n&\\sim\n\\text{N}_{p}(\\bar{\\Bbeta}_n\\com\\sigma^2\\BH^{-1}_n)\n\\\\\n\\\\\n\\tilde{y}&=\\tilde{\\Bx}^\\tr\\Bbeta+\\tilde{\\varepsilon},\\quad \\tilde{\\varepsilon}\\sim\\text{N}(0\\com\\sigma^2).\n\\end{aligned}\n\\]\n\n\nThe posterior predictive distribution is the marginal:\n\n\n\\[\n\\begin{aligned}\np(\\tilde{y}\\mid \\tilde{\\Bx}\\com y_{1:n}\\com \\Bx_{1:n})\n&=\n\\int\\int\np(\\tilde{y}\\com\\sigma^2\\com\\Bbeta\\mid \\tilde{\\Bx}\\com y_{1:n}\\com \\Bx_{1:n})\n\\,\\dd\\Bbeta\\,\\dd\\sigma^2\n\\\\\n&=\n\\int\\int\n\\underbrace{p(\\tilde{y}\\mid \\tilde{\\Bx}\\com \\sigma^2\\com\\Bbeta)}_{\\N(\\tilde{\\Bx}^\\tr\\Bbeta\\com\\sigma^2)}\n\\underbrace{p(\\sigma^2\\com\\Bbeta\\mid y_{1:n}\\com \\Bx_{1:n})}_{\\text{NIG}_{p}(a_n\\com b_n\\com \\bar{\\Bbeta}_n\\com\\BH_n)}\n\\,\\dd\\Bbeta\\,\\dd\\sigma^2.\n\\end{aligned}\n\\]\n\n\nWe can actually solve this."
  },
  {
    "objectID": "slides/case-study-2.html#first-marginalize-out-bbeta",
    "href": "slides/case-study-2.html#first-marginalize-out-bbeta",
    "title": "Case Study 2",
    "section": "First: marginalize out \\(\\Bbeta\\)\n",
    "text": "First: marginalize out \\(\\Bbeta\\)\n\nWe know that\n\\[\n\\begin{aligned}\n\\Bbeta\\mid \\sigma^2\\com y_{1:n}\\com \\Bx_{1:n}\n&\\sim\n\\text{N}_{p}(\\bar{\\Bbeta}_n\\com\\sigma^2\\BH^{-1}_n)\n\\\\\n\\tilde{y}\n&=\n\\tilde{\\Bx}^\\tr\\Bbeta\n+\n\\tilde{\\varepsilon}\n,\n&&\n\\tilde{\\varepsilon}\\sim\\N(0\\com \\sigma^2).\n\\end{aligned}\n\\]\n\nPre-multiplying by \\(\\tilde{\\Bx}^\\tr\\) is just a linear transformation, so:\n\\[\n\\tilde{\\Bx}^\\tr\\Bbeta\n\\mid \\tilde{\\Bx}\\com \\sigma^2\\com y_{1:n}\\com \\Bx_{1:n}\n\\sim\n\\N(\n\\tilde{\\Bx}^\\tr\\bar{\\Bbeta}_n\n\\com\n\\sigma^2\\tilde{\\Bx}^\\tr\\BH_n^{-1}\\tilde{\\Bx}\n).\n\\]\n\n\nSince \\(\\tilde{y}\\) is the sum of two independent normal bits (\\(\\tilde{\\Bx}^\\tr\\Bbeta\\) and \\(\\tilde{\\varepsilon}\\)), it stays normal, and you can add the means and variances:\n\n\n\\[\n\\begin{aligned}\n\\tilde{y}\\mid \\tilde{\\Bx}\\com \\sigma^2\\com y_{1:n}\\com \\Bx_{1:n}\n&\\sim\n\\N\\left(\n\\tilde{\\Bx}^\\tr\\bar{\\Bbeta}_n+0\n\\com\n\\sigma^2\\tilde{\\Bx}^\\tr\\BH_n^{-1}\\tilde{\\Bx}+\\sigma^2\n\\right)\n\\\\\n&\\sim\n\\N\\left(\n\\tilde{\\Bx}^\\tr\\bar{\\Bbeta}_n\n\\com\n\\sigma^2(1+\\tilde{\\Bx}^\\tr\\BH_n^{-1}\\tilde{\\Bx})\n\\right).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/case-study-2.html#second-marginalize-out-sigma2",
    "href": "slides/case-study-2.html#second-marginalize-out-sigma2",
    "title": "Case Study 2",
    "section": "Second: marginalize out \\(\\sigma^2\\)\n",
    "text": "Second: marginalize out \\(\\sigma^2\\)\n\nWe know that\n\\[\n\\begin{aligned}\n\\sigma^2\\mid y_{1:n}\\com \\Bx_{1:n}\n&\\sim\n\\text{IG}(a_n\\com b_n)\n\\\\\n\\tilde{y}\\mid \\tilde{\\Bx}\\com \\sigma^2\\com y_{1:n}\\com \\Bx_{1:n}\n&\\sim\n\\N\\left(\n\\tilde{\\Bx}^\\tr\\bar{\\Bbeta}_n\n\\com\n\\sigma^2(1+\\tilde{\\Bx}^\\tr\\BH_n^{-1}\\tilde{\\Bx})\n\\right).\n\\end{aligned}\n\\]\n\nMarginalizing \\(\\sigma^2\\) out of this hierarchy is essentially the definition of Student’s \\(t\\):\n\n\n\\[\n\\begin{aligned}\n\\tilde{y}\\mid \\tilde{\\Bx}\\com y_{1:n}\\com \\Bx_{1:n}\n&\\sim\nt(\\nu_n\\com\\bar{y}_{n}\\com s_{n}^2)\n\\\\\n\\\\\n\\nu_{n}\n&=\n2a_n\n\\\\\n\\bar{y}_{n}\n&=\n\\tilde{\\Bx}^\\tr\\bar{\\Bbeta}_n\n\\\\\ns_{n}^2\n&=\n\\frac{b_n}{a_n}\n(1+\\tilde{\\Bx}^\\tr\\BH_n^{-1}\\tilde{\\Bx})\n.\n\\end{aligned}\n\\]\n\n\nSo, Student’s \\(t\\) with a shift and scale. See the slides at the very end for the derivation."
  },
  {
    "objectID": "slides/case-study-2.html#comparison-1",
    "href": "slides/case-study-2.html#comparison-1",
    "title": "Case Study 2",
    "section": "Comparison",
    "text": "Comparison\n\n\nClassical prediction interval:\n\\[\n\\begin{aligned}\n\\hat{y} &\\pm t^\\star_{n-2} \\times \\text{SE}\n\\\\\n\\\\\n\\hat{y}\n&=\n\\hat{\\beta_0}+\\hat{\\beta}_1\\tilde{x}\n\\\\\n\\text{SE}^2\n&=\n\\widehat{\\sigma^2}\\left(1+\\frac{1}{n}+\\frac{(\\tilde{x}-\\bar{x})^2}{S_{xx}}\\right).\n\\end{aligned}\n\\]\n\nPosterior predictive distribution:\n\\[\n\\begin{aligned}\n\\tilde{y}\\mid \\tilde{\\Bx}\\com y_{1:n}\\com \\Bx_{1:n}\n&\\sim\nt(\\nu_n\\com\\bar{y}_{n}\\com s_{n}^2)\n\\\\\n\\\\\n\\nu_{n}\n&=\n2a_n\n\\\\\n\\bar{y}_{n}\n&=\n\\tilde{\\Bx}^\\tr\\bar{\\Bbeta}_n\n\\\\\ns_{n}^2\n&=\n\\frac{b_n}{a_n}\n(1+\\tilde{\\Bx}^\\tr\\BH_n^{-1}\\tilde{\\Bx})\n.\n\\end{aligned}\n\\]\n\n\nBoth centered at point prediction;\nPoint prediction is \\(\\tilde{\\Bx}\\) times point estimate, be it OLS or posterior mean;\nBoth scaled by a factor that incorporates uncertainty from \\(\\tilde{\\varepsilon}\\) and estimation uncertainty for \\(\\Bbeta\\), be it frequentist or Bayesian;\nBoth use Student’s \\(t\\) because \\(\\sigma^2\\) also had to be estimated;\nAs \\(n\\to\\infty\\), these will actually agree thanks to the Bernstein–von Mises theorem."
  },
  {
    "objectID": "slides/case-study-2.html#you-need-to-be-thinking-about-this",
    "href": "slides/case-study-2.html#you-need-to-be-thinking-about-this",
    "title": "Case Study 2",
    "section": "You need to be thinking about this",
    "text": "You need to be thinking about this\n\nWhether you take a classical or a Bayesian approach;\nWhether you’re using the linear model or some ML behemoth;\nYou need to produce and evaluate predictive uncertainty.\n\n\nStay tuned, and we’ll show you how!"
  },
  {
    "objectID": "slides/case-study-2.html#before-lab-on-friday",
    "href": "slides/case-study-2.html#before-lab-on-friday",
    "title": "Case Study 2",
    "section": "Before lab on Friday",
    "text": "Before lab on Friday\nSkim some background reading:\n\nAruoba (2008 JMCB): “Data revisions are not well behaved;”\nCroushore (2011 JEL): “Frontiers of real-time data analysis;”\nJacobs and van Norden (2016 JoM): “Why are initial estimates of productivity growth so unreliable?”\n\nCheck out FRED and the FRBP’s Real-Time Data Set."
  },
  {
    "objectID": "slides/case-study-2.html#heres-an-old-friend",
    "href": "slides/case-study-2.html#heres-an-old-friend",
    "title": "Case Study 2",
    "section": "Here’s an old friend",
    "text": "Here’s an old friend\nIf \\(X\\sim t_\\nu\\) has Student’s \\(t\\)-distribution, then its density is\n\\[\nf_X(x)=\\frac{\\Gamma \\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\pi\\nu}\\, \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}},\\quad x\\in\\mathbb{R}.\n\\]"
  },
  {
    "objectID": "slides/case-study-2.html#non-standard-students-t",
    "href": "slides/case-study-2.html#non-standard-students-t",
    "title": "Case Study 2",
    "section": "Non-standard Student’s \\(t\\)\n",
    "text": "Non-standard Student’s \\(t\\)\n\nIf you define a location-scale transformation \\(Y=\\mu+\\tau X\\) for constants \\(\\mu\\in\\mathbb{R}\\) and \\(\\tau&gt;0\\), then the new random variable \\(Y\\sim t(\\nu,\\,\\mu,\\,\\tau^2)\\) has a non-standard Student’s \\(t\\)-distribution. By the change-of-variables formula, the new density is\n\\[\nf_Y(y)\n=\n\\frac{1}{\\tau}f_X\\left(\\frac{y-\\mu}{\\tau}\\right)\n=\n\\frac{1}{\\tau}\n\\frac{\\Gamma{\\left(\\frac{\\nu+1}{2}\\right)}}{\\sqrt{\\pi\\nu}\\,\\Gamma{\\left(\\frac{\\nu}{2}\\right)}} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{y-\\mu}{\\tau}\\right)^2\\right)^{-(\\nu + 1)/2}.\n\\]\nBe careful not to immediately interpret \\(\\mu\\) and \\(\\tau^2\\) as mean and variance. Only if \\(\\nu&gt;1\\) is \\(E(Y)=\\mu\\), and \\(\\tau^2\\) is only proportional to the variance if \\(\\nu&gt;2\\)."
  },
  {
    "objectID": "slides/case-study-2.html#whence-students-t",
    "href": "slides/case-study-2.html#whence-students-t",
    "title": "Case Study 2",
    "section": "Whence Student’s \\(t\\)?",
    "text": "Whence Student’s \\(t\\)?\n\n\n\n\n\n\nTheorem\n\n\nConsider a bivariate distribution \\(p(y,\\,\\sigma^2)\\) written hierarchically:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\n\\sim\n\\text{IG}(a,\\,b)\n\\\\\ny\n\\,|\\,\n\\sigma^2\n&\n\\sim\n\\text{N}\n(m\n,\\,\n\\sigma^2v^2)\n.\n\\end{aligned}\n\\]\nIf you integrate \\(\\sigma^2\\) out, the marginal distribution is \\(y\\sim t\\left(2a,\\,m,\\,\\frac{b}{a}v^2\\right)\\).\n\n\n\nThis is precisely the sort of hierarchy we get when we consider the posterior predictive distribution in the linear model with conjugate prior!"
  },
  {
    "objectID": "slides/case-study-2.html#gird-your-loins",
    "href": "slides/case-study-2.html#gird-your-loins",
    "title": "Case Study 2",
    "section": "Gird your loins!",
    "text": "Gird your loins!\nIntegrate \\(\\sigma^2\\) out:\n\\[\n\\begin{aligned}\np(y)\n&=\n\\int_0^\\infty p(y,\\,\\sigma^2)\\,\\text{d}\\sigma^2\n\\\\\n&=\n\\int_0^\\infty p(y\\mid\\sigma^2)p(\\sigma^2)\\,\\text{d}\\sigma^2\n\\\\\n&=\n\\int_0^\\infty \\underbrace{\\frac{1}{\\sqrt{2\\pi\\sigma^2v^2}}\\exp\\left(-\\frac{(y-m)^2}{2\\sigma^2v^2}\\right)}_{\\text{normal pdf}}\\underbrace{\\frac{b^a}{\\Gamma(a)}(\\sigma^2)^{-a-1}\\exp\\left(-b/\\sigma^2\\right)}_{\\text{inverse gamma pdf}}\\,\\dd\\sigma^2\n\\\\\n&=\n\\frac{b^a}{\\Gamma(a)\\sqrt{2\\pi v^2}}\n\\int_0^\\infty (\\sigma^2)^{-1/2}\\exp\\left(-\\frac{(y-m)^2}{2\\sigma^2v^2}\\right)(\\sigma^2)^{-a-1}\\exp\\left(-b/\\sigma^2\\right)\\,\\dd\\sigma^2\n\\\\\n&=\n\\frac{b^a}{\\Gamma(a)\\sqrt{2\\pi v^2}}\n\\int_0^\\infty \\underbrace{(\\sigma^2)^{-(a+1/2)-1}\\exp\\left(-\\left[b+\\frac{(y-m)^2}{2v^2}\\right]\\frac{1}{\\sigma^2}\\right)}_{\\text{kernel of IG}(a+1/2,\\,b+(y-m)^2/2v^2)}\\,\\dd\\sigma^2\n\\\\\n&=\n\\frac{b^a}{\\Gamma(a)\\sqrt{2\\pi v^2}}\n\\frac{\\Gamma\\left(a+\\frac{1}{2}\\right)}{\\left(b+\\frac{(y-m)^2}{2v^2}\\right)^{a+1/2}}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/case-study-2.html#gird-your-loins-1",
    "href": "slides/case-study-2.html#gird-your-loins-1",
    "title": "Case Study 2",
    "section": "Gird your loins!",
    "text": "Gird your loins!\nNow let’s jerk it around until we recognize that it’s (non-standard) Student’s \\(t\\):\n\\[\n\\begin{aligned}\np(y)\n&=\n\\frac{b^a}{\\Gamma(a)\\sqrt{2\\pi v^2}}\n\\frac{\\Gamma\\left(a+\\frac{1}{2}\\right)}{\\left(b+\\frac{(y-m)^2}{2v^2}\\right)^{a+1/2}}\n\\\\\n&=\n\\frac{\\Gamma\\left(\\frac{2a+1}{2}\\right)}{\\Gamma\\left(\\frac{2a}{2}\\right)\\sqrt{\\pi 2 v^2}}\nb^a\n\\left(b+\\frac{(y-m)^2}{2v^2}\\right)^{-(a+1/2)}\n\\\\\n&=\n\\frac{\\Gamma\\left(\\frac{2a+1}{2}\\right)}{\\Gamma\\left(\\frac{2a}{2}\\right)\\sqrt{\\pi 2 \\frac{a}{a}v^2}}\nb^a\n\\left(b+\\frac{(y-m)^2}{2\\frac{a}{a}v^2}\\right)^{-(a+1/2)}\n\\\\\n&=\n\\frac{\\Gamma\\left(\\frac{2a+1}{2}\\right)}{\\Gamma\\left(\\frac{2a}{2}\\right)\\sqrt{\\pi 2 a}}\n\\frac{b^a}{\\sqrt{v^2/a}}\n\\left(b+\\frac{(y-m)^2}{2av^2/a}\\right)^{-(a+1/2)}\n\\\\\n&=\n\\frac{\\Gamma\\left(\\frac{2a+1}{2}\\right)}{\\Gamma\\left(\\frac{2a}{2}\\right)\\sqrt{\\pi 2 a}}\n\\frac{b^a}{\\sqrt{v^2/a}}\n\\left[b\\left(1+\\frac{(y-m)^2}{2abv^2/a}\\right)\\right]^{-(a+1/2)}\n\\\\\n&=\n\\frac{\\Gamma\\left(\\frac{2a+1}{2}\\right)}{\\Gamma\\left(\\frac{2a}{2}\\right)\\sqrt{\\pi 2 a}}\n\\frac{b^a}{\\sqrt{v^2/a}}\nb^{-(a+1/2)}\\left(1+\\frac{1}{2a}\\left(\\frac{y-m}{\\sqrt{bv^2/a}}\\right)^2\\right)^{-(a+1/2)}\n\\\\\n&=\n\\frac{\\Gamma\\left(\\frac{2a+1}{2}\\right)}{\\Gamma\\left(\\frac{2a}{2}\\right)\\sqrt{\\pi 2 a}}\n\\frac{b^a}{b^ab^{1/2}\\sqrt{v^2/a}}\n\\left(1+\\frac{1}{2a}\\left(\\frac{y-m}{\\sqrt{bv^2/a}}\\right)^2\\right)^{-(a+1/2)}\n\\\\\n&=\n\\frac{1}{\\sqrt{bv^2/a}}\n\\frac{\\Gamma\\left(\\frac{2a+1}{2}\\right)}{\\Gamma\\left(\\frac{2a}{2}\\right)\\sqrt{\\pi 2 a}}\n\\left(1+\\frac{1}{2a}\\left(\\frac{y-m}{\\sqrt{bv^2/a}}\\right)^2\\right)^{-(a+1/2)}\n\\\\\n&=\n\\frac{1}{\\sqrt{bv^2/a}}\n\\frac{\\Gamma\\left(\\frac{2a+1}{2}\\right)}{\\Gamma\\left(\\frac{2a}{2}\\right)\\sqrt{\\pi 2 a}}\n\\left(1+\\frac{1}{2a}\\left(\\frac{y-m}{\\sqrt{bv^2/a}}\\right)^2\\right)^{-(2a+1)/2)}.\n\\end{aligned}\n\\]\nIf you started with \\(X\\sim t_{2a}\\) and then defined \\(Y=m+\\sqrt{\\frac{b}{a}v^2}X\\), that’s the density you would get."
  },
  {
    "objectID": "slides/more-ts.html#time-series-models",
    "href": "slides/more-ts.html#time-series-models",
    "title": "Time series potpourri",
    "section": "Time series models",
    "text": "Time series models\n\nA time series is a set of measurements collected over time;\nWe model these data as a sequence of dependent random variables:\n\n\\[\n\\mathbf{y}_{0:T} = \\{\\mathbf{y}_0,\\,\\mathbf{y}_1,\\,\\mathbf{y}_2,\\,...,\\,\\mathbf{y}_T\\}.\n\\]\n\nA time series model is “just” their joint probability distribution:\n\n\\[\np(\\mathbf{y}_{0:T}) = p(\\mathbf{y}_0)\\prod_{t=1}^Tp(\\mathbf{y}_t\\,|\\,\\mathbf{y}_{0:t-1}).\n\\]"
  },
  {
    "objectID": "slides/more-ts.html#the-autoregression-of-order-p",
    "href": "slides/more-ts.html#the-autoregression-of-order-p",
    "title": "Time series potpourri",
    "section": "The autoregression of order \\(p\\)\n",
    "text": "The autoregression of order \\(p\\)\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\beta_2\ny_{t-2}\n+\n\\cdots\n+\n\\beta_p\ny_{t-p}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2).\n\\end{aligned}\n\\]\n\nThis implies a joint distribution written marginal-conditional style:\n\\[\n\\begin{aligned}\np(y_{1:T}\\mid y_0\\com y_{-1}\\com ...\\com y_{1-p})\n&=\n\\prod_{t=1}^Tp(y_t\\mid y_{1-p:t-1})\\\\\n\\\\\n&=\n\\prod_{t=1}^Tp(y_t\\mid y_{t-p:t-1}).\n\\end{aligned}\n\\]\n\n\nThe joint distribution is just a big ol’ multivariate normal with a structured mean and covariance that depend on the parameters."
  },
  {
    "objectID": "slides/more-ts.html#estimation",
    "href": "slides/more-ts.html#estimation",
    "title": "Time series potpourri",
    "section": "Estimation",
    "text": "Estimation\nSimilar to iid regression:\n\nClassical approach: MLE = OLS;\nBayesian approach: normal-inverse-gamma conjugate prior.\n\nBut the inferential theory is very different!"
  },
  {
    "objectID": "slides/more-ts.html#probabilistic-prediction",
    "href": "slides/more-ts.html#probabilistic-prediction",
    "title": "Time series potpourri",
    "section": "Probabilistic prediction",
    "text": "Probabilistic prediction\nTo generate interval and density predictions \\(h\\)-steps-ahead:\n\nClassical approach: bootstrap;\nBayesian approach: posterior predictive simulation.\n\nThese incorporate data and parameter estimation uncertainty."
  },
  {
    "objectID": "slides/more-ts.html#hyperparameter-tuning",
    "href": "slides/more-ts.html#hyperparameter-tuning",
    "title": "Time series potpourri",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\nThe lag order \\(p\\) is a hyperparameter that must be selected:\n\neyeball ACF plots;\noptimize canned criteria like AIC or BIC;\ntime series cross validation."
  },
  {
    "objectID": "slides/more-ts.html#why",
    "href": "slides/more-ts.html#why",
    "title": "Time series potpourri",
    "section": "Why?",
    "text": "Why?\nWe have seen this so far:\n\\[\ny_t\n=\n\\beta_0\n+\n\\sum\\limits_{l=1}^p\\beta_ly_{t-l}\n+\n\\underbrace{\nu_t\n}_{\\text{iid}}\n.\n\\]\nWhat if there is time series dependence in the error term as well?\n\nWhat kind?"
  },
  {
    "objectID": "slides/more-ts.html#autoregressive-moving-average-arma",
    "href": "slides/more-ts.html#autoregressive-moving-average-arma",
    "title": "Time series potpourri",
    "section": "Autoregressive moving average (ARMA)",
    "text": "Autoregressive moving average (ARMA)\nARMA(\\(p\\), \\(q\\)):\n\\[\ny_t\n=\n\\beta_0\n+\n\\underbrace{\\sum\\limits_{l=1}^p\\beta_ly_{t-l}}_{\\text{autoregressive}}\n+\n\\underbrace{\\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}}_{\\text{moving average}}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\iid\n\\N(0\\com \\sigma^2)\n.\n\\]\n\nFlexible class of models that can capture pretty generic linear dependencies.\n\n\n\\(p=q=0\\) gives the iid model;\n\n\\(q=0\\) gives the pure AR(\\(p\\)) we’ve already seen;\n\n\\(p=0\\) gives the pure MA(\\(q\\)), which is seldom used on its own."
  },
  {
    "objectID": "slides/more-ts.html#what-is-the-joint-distribution",
    "href": "slides/more-ts.html#what-is-the-joint-distribution",
    "title": "Time series potpourri",
    "section": "What is the joint distribution?",
    "text": "What is the joint distribution?\nIt’s normal! The mean and covariance depend on the AR and MA parameters and the error variance:\n\\[\n\\begin{bmatrix}\ny_1 & y_2 & \\cdots & y_T\n\\end{bmatrix}^\\tr\n\\sim\\N_T\\left(\\Bmu\\com \\BSigma\\right).\n\\]\nDon’t worry about the details, but there are theorems that say the ARMA model is flexible enough to capture any stable, linear time series dependence if \\(p\\) and \\(q\\) are sufficiently large."
  },
  {
    "objectID": "slides/more-ts.html#graphical-structure",
    "href": "slides/more-ts.html#graphical-structure",
    "title": "Time series potpourri",
    "section": "Graphical structure",
    "text": "Graphical structure\nConsider the ARMA(1, 1):\n\\[\ny_t=\\beta_0+\\beta_1y_{t-1}+\\theta_1\\varepsilon_{t-1}+\\varepsilon_t, \\quad \\varepsilon_t\\iid\\N(0\\com \\sigma^2).\n\\]\nThen:\n\\[\n\\begin{matrix}\n  y_0 & \\to & y_1 & \\to & y_2 & \\to & y_3 & \\to & y_4 & \\to &\\cdots \\\\\n   & \\nearrow & \\uparrow & \\nearrow & \\uparrow & \\nearrow & \\uparrow & \\nearrow & \\uparrow & \\nearrow &  \\\\\n  \\varepsilon_0 &  & \\varepsilon_1 &  & \\varepsilon_2 &  & \\varepsilon_3 &  & \\varepsilon_4 &  & \\\\\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "slides/more-ts.html#arma2-2-distribution",
    "href": "slides/more-ts.html#arma2-2-distribution",
    "title": "Time series potpourri",
    "section": "ARMA(2, 2) distribution",
    "text": "ARMA(2, 2) distribution\nAPP"
  },
  {
    "objectID": "slides/more-ts.html#autocovariance",
    "href": "slides/more-ts.html#autocovariance",
    "title": "Time series potpourri",
    "section": "Autocovariance",
    "text": "Autocovariance\n\nThe moving average piece is always stable;\nIf the AR piece is stable as well, then we have a shift-invariant autocovariance function\n\n\\[\n\\gamma(h)=\\cov(y_{t+h}\\com y_{t}).\n\\]\n\nThe formulas aren’t pretty, but a computer can handle it easily;\nWe know an AR(1) is stable if \\(|\\beta_1|&lt;1\\). For the general AR(p), it’s complicated."
  },
  {
    "objectID": "slides/more-ts.html#acf-of-the-arma",
    "href": "slides/more-ts.html#acf-of-the-arma",
    "title": "Time series potpourri",
    "section": "ACF of the ARMA",
    "text": "ACF of the ARMA\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nplot_arma_acf &lt;- function(ar = numeric(), ma = numeric(), lag.max = 10){\n  y_vals &lt;- ARMAacf(ar = ar, ma = ma, lag.max = lag.max)\n  plot(0:lag.max, y_vals, pch = 19, ylim = c(-1, 1),\n       xlab = \"h\", ylab = expression(rho~\"(h)\"))\n  segments(0:lag.max, 0, 0:lag.max, y_vals)\n  abline(h = 0, col = \"lightgrey\")\n}\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"Theoretical autocorrelation of a stationary ARMA(2, 2)\"),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b1\",\n                  \"β₁\",\n                  min = -2,\n                  max = 2,\n                  value = 0,\n                  step = 0.05),\n      sliderInput(\"b2\",\n                  \"β₂\",\n                  min = -2,\n                  max = 2,\n                  value = 0,\n                  step = 0.05),\n      sliderInput(\"th1\",\n                  \"θ₁\",\n                  min = -0.99,\n                  max = 0.99,\n                  value = 0,\n                  step = 0.01),\n      sliderInput(\"th2\",\n                  \"θ₁\",\n                  min = -0.99,\n                  max = 0.99,\n                  value = 0,\n                  step = 0.01),\n      sliderInput(\"H\",\n                  \"H\",\n                  min = 0,\n                  max = 50,\n                  step = 1,\n                  value = 20)\n    ),\n    \n    # Show a plot of the generated distribution\n    mainPanel(\n            plotOutput(\"acf\", height = \"400px\"),\n      plotOutput(\"stationarityPlot\", height = \"400px\"),\n      verbatimTextOutput(\"stationarity\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  output$acf &lt;- renderPlot({\n    b1 &lt;- input$b1\n    b2 &lt;- input$b2\n    th1 &lt;- input$th1\n    th2 &lt;- input$th2\n    H &lt;- input$H\n    \n    plot_arma_acf(ar = c(b1, b2), ma = c(th1, th2), lag.max = H)\n\n  })\n  \n    output$stationarityPlot &lt;- renderPlot({\n    b1 &lt;- input$b1\n    b2 &lt;- input$b2\n    \n    # triangle vertices\n    tri_x &lt;- c(2, -2, 0)\n    tri_y &lt;- c(-1, -1, 1)\n    par(mar = c(4, 4, 0.1, 4))\n    plot(tri_x, tri_y, type = \"n\",\n         xlab = expression(beta[1]),\n         ylab = expression(beta[2]),\n         xlim = c(-2.1, 2.1),\n         ylim = c(-1.1, 1.1),\n         xaxt = \"n\", yaxt = \"n\", bty = \"n\")\n    axis(1, at = c(-2, 0, 2), pos = 0)\n    axis(2, at = c(-1, 0, 1), pos = 0)\n    \n    polygon(tri_x, tri_y, col = rgb(0, 0, 1, 0.2), border = \"blue\", lwd = 2)\n    points(b1, b2, pch = 19, col = \"red\", cex = 1.5)\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "slides/more-ts.html#estimation-1",
    "href": "slides/more-ts.html#estimation-1",
    "title": "Time series potpourri",
    "section": "Estimation",
    "text": "Estimation\nIt’s a mess:\n\nThe maximum likelihood problem for ARMA must be solved numerically;\nThere is no conjugate prior, and MCMC is a bit of a nightmare here.\n\nBase R ships with some basic tools."
  },
  {
    "objectID": "slides/more-ts.html#simulate-an-arma",
    "href": "slides/more-ts.html#simulate-an-arma",
    "title": "Time series potpourri",
    "section": "Simulate an ARMA",
    "text": "Simulate an ARMA\nThis is a function in base:\n\nset.seed(8675309)\n\nT &lt;- 200\ny &lt;- arima.sim(n = T, model = list(ar = 0.99, ma = -0.4), sd = sqrt(2))\n\nplot(y)"
  },
  {
    "objectID": "slides/more-ts.html#fit-an-arma-numerically",
    "href": "slides/more-ts.html#fit-an-arma-numerically",
    "title": "Time series potpourri",
    "section": "Fit an ARMA numerically",
    "text": "Fit an ARMA numerically\nThis is a function in base:\n\nmy_arma_fit &lt;- arima(y, order = c(1, 0, 1))\n\nmy_arma_fit\n\n\nCall:\narima(x = y, order = c(1, 0, 1))\n\nCoefficients:\n         ar1      ma1  intercept\n      0.9871  -0.4307    -2.2742\ns.e.  0.0089   0.0595     3.4451\n\nsigma^2 estimated as 2.131:  log likelihood = -360.85,  aic = 729.69\n\n\nNot far from the true values we provided. It’s tougher mathematically than iid regression, but we have asymptotic statistical theory for ARMA models (even when they’re not stable!)."
  },
  {
    "objectID": "slides/more-ts.html#predict",
    "href": "slides/more-ts.html#predict",
    "title": "Time series potpourri",
    "section": "Predict",
    "text": "Predict\nThis is a function in base:\n\nH &lt;- 30\nfc &lt;- predict(my_arma_fit, n.ahead = H)\n\nhead(fc$pred) # point prediction\n\nTime Series:\nStart = 201 \nEnd = 206 \nFrequency = 1 \n[1] -1.000734 -1.017113 -1.033281 -1.049241 -1.064996 -1.080549\n\nhead(fc$se) # predictive standard error\n\nTime Series:\nStart = 201 \nEnd = 206 \nFrequency = 1 \n[1] 1.459916 1.670697 1.853169 2.015144 2.161334 2.294846"
  },
  {
    "objectID": "slides/more-ts.html#fan-chart-by-hand",
    "href": "slides/more-ts.html#fan-chart-by-hand",
    "title": "Time series potpourri",
    "section": "Fan chart by-hand",
    "text": "Fan chart by-hand\n\nplot(y, xlim = c(0, T + H))\nlines(fc$pred, col = \"red\")\nlines(fc$pred + 1.96 * fc$se, col = \"red\")\nlines(fc$pred - 1.96 * fc$se, col = \"red\")"
  },
  {
    "objectID": "slides/more-ts.html#the-hyndmanverse",
    "href": "slides/more-ts.html#the-hyndmanverse",
    "title": "Time series potpourri",
    "section": "The hyndmanverse",
    "text": "The hyndmanverse\n\n\n\n\n\nTextbook is free online;\nVery popular is some quarters;\nAccompanying software is inspired by the tidyverse and very well-documented:\n\n\nlibrary(tsibble)\nlibrary(fable)"
  },
  {
    "objectID": "slides/more-ts.html#only-includes-data-uncertainty-no-bootstrap",
    "href": "slides/more-ts.html#only-includes-data-uncertainty-no-bootstrap",
    "title": "Time series potpourri",
    "section": "Only includes data uncertainty (no bootstrap!)",
    "text": "Only includes data uncertainty (no bootstrap!)\n\nmy_data = tsibble(period = 1:T, y = y, index = period)  \n\nmy_data |&gt;\n  model(my_model = ARIMA(y ~ 1 + pdq(1, 0, 1))) |&gt;\n  forecast(h = 30) |&gt;\n  autoplot(my_data)"
  },
  {
    "objectID": "slides/more-ts.html#data-and-parameter-estimation-uncertainty",
    "href": "slides/more-ts.html#data-and-parameter-estimation-uncertainty",
    "title": "Time series potpourri",
    "section": "Data and parameter estimation uncertainty",
    "text": "Data and parameter estimation uncertainty\nNot much difference in this case\n\nmy_data |&gt;\n  model(my_model = ARIMA(y ~ 1 + pdq(1, 0, 1))) |&gt;\n  forecast(h = 30, bootstrap = TRUE, times = 5000) |&gt;\n  autoplot(my_data)"
  },
  {
    "objectID": "slides/more-ts.html#state-space-models-ssms",
    "href": "slides/more-ts.html#state-space-models-ssms",
    "title": "Time series potpourri",
    "section": "State space models (SSMs)",
    "text": "State space models (SSMs)\nState space models are a meta-family of models that nest as special cases most time series models you will encounter in practice. SSMs are…\n\ntime series models;\ngraphical models;\nlatent variable models;\nhierarchical models;\nBayesian models.\n\n\nThis makes them really rich and interesting! Anything you know about those separate areas of statistics helps with studying SSMs."
  },
  {
    "objectID": "slides/more-ts.html#the-main-idea",
    "href": "slides/more-ts.html#the-main-idea",
    "title": "Time series potpourri",
    "section": "The main idea",
    "text": "The main idea\n\nState space models are first and foremost latent variable models;\nThey treat the observed time series \\(\\By_t\\) as a noisy or corrupted version of an unobserved (or latent) time series \\(\\Bs_t\\);\nThe goal of inference is to use what we observed (\\(\\By_t\\)) to learn about what we didn’t observe (\\(\\Bs_t\\));\nThe is usually done from a Bayesian point of view, so learning means accessing \\(p(\\Bs_{1:T}\\mid \\By_{1:T})\\);\nThere are usually unobserved parameters \\(\\Btheta\\) floating around that we also have to deal with."
  },
  {
    "objectID": "slides/more-ts.html#example-signal-in-noise",
    "href": "slides/more-ts.html#example-signal-in-noise",
    "title": "Time series potpourri",
    "section": "Example: signal-in-noise",
    "text": "Example: signal-in-noise\n\n\nThe \\(y_t\\) we got to see is a corrupted version of some smooth underlying trend or signal \\(s_t\\) that we wish to extract:\n\\[\ny_t=s_t+\\varepsilon_t,\\quad \\varepsilon_t\\iid\\N(0\\com \\sigma^2).\n\\]"
  },
  {
    "objectID": "slides/more-ts.html#example-stochastic-volatility",
    "href": "slides/more-ts.html#example-stochastic-volatility",
    "title": "Time series potpourri",
    "section": "Example: stochastic volatility",
    "text": "Example: stochastic volatility\n\n\nThe variance of \\(y_t\\) is changing over time:\n\\[\ny_t=\\mu+\\varepsilon_t,\\quad \\varepsilon_t\\indep\\N(0\\com \\sigma_t^2).\n\\]"
  },
  {
    "objectID": "slides/more-ts.html#example-regime-switching",
    "href": "slides/more-ts.html#example-regime-switching",
    "title": "Time series potpourri",
    "section": "Example: regime-switching",
    "text": "Example: regime-switching\n\n\nThe distribution of \\(y_t\\) is abruptly switching between a discrete set of options:\n\\[\ny_t\n\\sim\n\\begin{cases}\n\\N(\\mu_1\\com \\sigma^2_1) & \\text{if }s_t=1\n\\\\\n\\N(\\mu_2\\com \\sigma^2_2) & \\text{if }s_t=2\n\\end{cases}\n\\]"
  },
  {
    "objectID": "slides/more-ts.html#example-time-varying-parameter-regression",
    "href": "slides/more-ts.html#example-time-varying-parameter-regression",
    "title": "Time series potpourri",
    "section": "Example: time-varying parameter regression",
    "text": "Example: time-varying parameter regression\nA time-varying linear relationship between response \\(y_t\\) and predictors \\(\\Bx_t\\):\n\\[\ny_t=\\Bx_t^\\tr\\Bbeta_t+\\varepsilon_t.\n\\]"
  },
  {
    "objectID": "slides/more-ts.html#zoom-out",
    "href": "slides/more-ts.html#zoom-out",
    "title": "Time series potpourri",
    "section": "Zoom out",
    "text": "Zoom out\nAll of those examples have the same basic structure:\n\nAn observed time series \\(y_t\\) depends on an unobserved time series.\n\nAlso! All of those models are non-stationary, which we should regard as the rule rather than the exception in time series."
  },
  {
    "objectID": "slides/more-ts.html#generic-structure",
    "href": "slides/more-ts.html#generic-structure",
    "title": "Time series potpourri",
    "section": "Generic structure",
    "text": "Generic structure\nBasic ingredients of all SSMs:\n\\[\n\\begin{aligned}\n    \\By_t&\\sim p(\\By_t\\given \\Bs_t\\com \\Btheta) && \\text{(measurement distribution)}\\\\\n    \\Bs_t&\\sim p(\\Bs_t\\given \\Bs_{t-1}\\com \\Btheta) && \\text{(state transition distribution)}\\\\\n    \\Bs_0&\\sim p(\\Bs_0\\given \\Btheta) && \\text{(initial condition)}\\\\\n    \\Btheta&\\sim p(\\Btheta). && \\text{(prior)}\n\\end{aligned}\n\\]\nGraphical structure:\n\\[\n    \\begin{matrix}\n\\Bs_0 & \\rightarrow & \\Bs_1 & \\rightarrow & \\Bs_2 & \\rightarrow & \\cdots & \\rightarrow & \\Bs_t & \\rightarrow & \\cdots \\\\\n& & \\downarrow & & \\downarrow & & & & \\downarrow & & \\\\\n  & & \\mathbf{y}_1 & & \\mathbf{y}_2 & & & & \\mathbf{y}_t & &\n\\end{matrix}\n\\]\nSo the \\(\\mathbf{y}_t\\) are conditionally independent. But this can be relaxed."
  },
  {
    "objectID": "slides/more-ts.html#dynamic-linear-model",
    "href": "slides/more-ts.html#dynamic-linear-model",
    "title": "Time series potpourri",
    "section": "Dynamic linear model",
    "text": "Dynamic linear model\n\\[\n\\begin{aligned}\n\\By_t\n&=\n\\BF\\Bs_t\n+\n\\Bepsilon_t,\n&&\n\\Bepsilon_t\\iid\\N_n(\\Bzero\\com \\BV)\n\\\\\n\\Bs_t\n&=\n\\BG\\Bs_{t-1}\n+\n\\Beta_t,\n&&\n\\Beta_t\\iid\\N_p(\\Bzero\\com \\BW)\n\\\\\n& &&\\\\\n\\Bs_0&\\sim\\N_p(\\bar{\\Bs}_{0|0}\\com \\BP_0).\n\\end{aligned}\n\\]\n\nSpecial cases: ARMA models, TVP regression, local-level model, etc;\nThe dynamics are governed by system matrices \\(\\BF\\), \\(\\BV\\), \\(\\BG\\), and \\(\\BW\\) which often depend on a lower-dimensional set of parameters \\(\\Btheta\\) that must be estimated."
  },
  {
    "objectID": "slides/more-ts.html#local-level-model",
    "href": "slides/more-ts.html#local-level-model",
    "title": "Time series potpourri",
    "section": "Local-level model",
    "text": "Local-level model\nThe simplest non-trivial special case:\n\\[\n\\begin{aligned}\ny_t\n&=\ns_t\n+\n\\varepsilon_t,\n&&\n\\varepsilon_t\\iid\\N(0\\com v)\n\\\\\ns_t\n&=\ns_{t-1}\n+\n\\eta_t,\n&&\n\\eta_t\\iid\\N(0\\com w)\n\\end{aligned}\n\\]\n\nThe classic “signal plus noise” model;\nWe set \\(\\BF=\\BG=1\\), and the static parameters \\(\\Btheta=(v\\com w)\\) control the smoothness and corruption of the signal."
  },
  {
    "objectID": "slides/more-ts.html#filtering-versus-smoothing",
    "href": "slides/more-ts.html#filtering-versus-smoothing",
    "title": "Time series potpourri",
    "section": "Filtering versus smoothing",
    "text": "Filtering versus smoothing\nTreat \\(\\Btheta=(v\\com w)\\) as fixed for now. There are two kinds of signal extraction:\n\n\n\\(p(s_t\\mid y_{1:t}\\com \\Btheta)\\): “one-sided” estimates of the trend given all data up to the present. This is called filtering;\n\n\\(p(s_t\\mid y_{1:T}\\com \\Btheta)\\): “two-sided”, retrospective estimates of the trend given all data past and present. This is called smoothing.\n\nIt’s all posterior inference for the latent variable. The only difference is when the estimate is computed and what data we are conditioning on."
  },
  {
    "objectID": "slides/more-ts.html#demo",
    "href": "slides/more-ts.html#demo",
    "title": "Time series potpourri",
    "section": "Demo",
    "text": "Demo\nCan we extract the smooth trend from data like these?\n\n\n\\[\ny_t\\indep\\N\\left(\\sin\\left(\\frac{t}{4}\\right)\\com \\sigma^2\\right).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nYes! But the parameters \\(\\Btheta=(v\\com w)\\) have a big influence."
  },
  {
    "objectID": "slides/more-ts.html#the-dlm-package",
    "href": "slides/more-ts.html#the-dlm-package",
    "title": "Time series potpourri",
    "section": "The dlm package",
    "text": "The dlm package\n\n\n\n\n\nCreated by Duke stats PhD alum Giovanni Petris;\nFree textbook PDF through Duke library;\nAccompanying software is well-documented:\n\n\nlibrary(dlm)"
  },
  {
    "objectID": "slides/more-ts.html#maximum-likelihood",
    "href": "slides/more-ts.html#maximum-likelihood",
    "title": "Time series potpourri",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nWhen it comes to estimating \\(v\\) and \\(w\\), \\(s_{1:T}\\) are nuisance parameters that can be fully integrated out using the Kalman filter:\n\\[\np(y_{1:T}\\mid v\\com w) = \\int p(y_{1:T}\\mid v\\com w\\com s_{1:T})\\,\\dd s_{1:T}.\n\\]\nThis is just a funky multivariate normal! Then we can compute maximum (marginal!) likelihood estimates using numerical optimization:\n\\[\n(\\hat{v}\\com \\hat{w})\n=\n\\underset{v\\com w}{\\arg\\max}\\,p(y_{1:T}\\mid v\\com w).\n\\]"
  },
  {
    "objectID": "slides/more-ts.html#example",
    "href": "slides/more-ts.html#example",
    "title": "Time series potpourri",
    "section": "Example",
    "text": "Example\nFake data:\n\nset.seed(8675309)\ny &lt;- sin((1:T) / 4) + rnorm(T, sd = 1)\n\nMLE in the dlm packages:\n\n# how do the parameters (log-variances) determine the model (local-level)\nmy_model &lt;- function(x) return(dlmModPoly(order = 1, dV = exp(x[1]), dW = exp(x[2])))\n\n# just passes to the optim function\nestimate &lt;- dlmMLE(y = y, parm = c(1, 1), build = my_model)\n\nexp(estimate$par) \n\n[1] 0.7460911 0.2590706"
  },
  {
    "objectID": "slides/more-ts.html#empirical-bayes-estimate-of-the-trend",
    "href": "slides/more-ts.html#empirical-bayes-estimate-of-the-trend",
    "title": "Time series potpourri",
    "section": "Empirical Bayes estimate of the trend",
    "text": "Empirical Bayes estimate of the trend\nThis is \\(E[s_{1:T}\\mid y_{1:T}\\com\\hat{\\Btheta}]\\):"
  },
  {
    "objectID": "slides/more-ts.html#bayesian-inference",
    "href": "slides/more-ts.html#bayesian-inference",
    "title": "Time series potpourri",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nPut conditionally conjugate priors on the unknown parameters:\n\\[\n\\begin{aligned}\ny_t\n&=\ns_t + \\varepsilon_t,\n&&\\varepsilon_t\\iid\\N(0\\com v)\n\\\\\ns_t &=s_{t-1}+\\eta_t,\n&&\\eta_t\\iid\\N(0\\com w)\n\\\\\nv&\\sim\\text{IG}(a_v\\com b_v)\n\\\\\nw&\\sim\\text{IG}(a_w\\com b_w)\n.\n\\end{aligned}\n\\]\nThe full posterior \\(p(s_{1:T}\\com v\\com w\\mid y_{1:T})\\) is intractable, but we can approximate it with a Gibbs sampler."
  },
  {
    "objectID": "slides/more-ts.html#gibbs-sampler",
    "href": "slides/more-ts.html#gibbs-sampler",
    "title": "Time series potpourri",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\nTo approximate \\(p(s_{1:T}\\com v\\com w\\mid y_{1:T})\\), cycle through simulating the full conditionals:\n\n\n\\(p(v \\mid w\\com s_{1:T}\\com y_{1:T})\\): this is inverse gamma;\n\n\\(p(w \\mid v\\com s_{1:T}\\com y_{1:T})\\): this is inverse gamma;\n\n\\(p(s_{1:T} \\mid v\\com w \\com y_{1:T})\\): this is multivariate normal.\n\nThe first two steps are very similar to STA 360/402. The last step is the Kalman smoother."
  },
  {
    "objectID": "slides/more-ts.html#example-1",
    "href": "slides/more-ts.html#example-1",
    "title": "Time series potpourri",
    "section": "Example",
    "text": "Example\nRun the Gibbs sampler:\n\nset.seed(525600)\nposterior_draws &lt;- dlmGibbsDIG(\n  y,\n  dlmModPoly(1),\n  shape.y = 1, rate.y = 1, \n  shape.theta = 1, rate.theta = 1, \n  n.sample = 5000,\n  progressBar = FALSE\n)\n\n\nGives you 5,000 draws of \\(\\{v\\com w\\com s_{1:T}\\}\\):\n\nposterior_draws$dV # draws of v\n\nposterior_draws$dW # draws of w\n\nposterior_draws$theta # draws of the states"
  },
  {
    "objectID": "slides/ar-intro.html#where-were-going",
    "href": "slides/ar-intro.html#where-were-going",
    "title": "Autoregressive models",
    "section": "Where we’re going",
    "text": "Where we’re going\n\nWe’ll introduce the most basic time series models;\n\nautoregression (AR);\ndynamic linear model (DLM);\n\nYou may or may not use them;\nThe basic models will provide a sandbox for illustrating probabilistic prediction and evaluation;\n\nYou will definitely use this!"
  },
  {
    "objectID": "slides/ar-intro.html#time-series",
    "href": "slides/ar-intro.html#time-series",
    "title": "Autoregressive models",
    "section": "Time series",
    "text": "Time series\n\nA time series is a set of measurements collected over time;\nWe model these data as a sequence of dependent random variables:\n\n\\[\n\\mathbf{y}_{0:T} = \\{\\mathbf{y}_0,\\,\\mathbf{y}_1,\\,\\mathbf{y}_2,\\,...,\\,\\mathbf{y}_T\\}.\n\\]\n\nA time series model is “just” their joint probability distribution:\n\n\\[\np(\\mathbf{y}_{0:T}) = p(\\mathbf{y}_0)\\prod_{t=1}^Tp(\\mathbf{y}_t\\,|\\,\\mathbf{y}_{0:t-1}).\n\\]\n\n\n\n\n\n\n\nStay grounded.\n\n\nLike much wisdom, that last bullet is simultaneously vacuous and profound. It tells you everything and it tells you nothing all at once. But don’t let this basic fact get lost in the sea of details."
  },
  {
    "objectID": "slides/ar-intro.html#notation-to-get-used-to",
    "href": "slides/ar-intro.html#notation-to-get-used-to",
    "title": "Autoregressive models",
    "section": "Notation to get used to",
    "text": "Notation to get used to\n\nI will not use uppercase \\(Y_t\\) versus lowercase \\(y_t\\) to distinguish random variables and fixed realizations. It’s all just \\(y_t\\), and context makes clear how it functions;\nA vector \\(\\mathbf{y}\\in\\mathbb{R}^n\\) is always an \\(n\\times 1\\) column. The corresponding row vector is \\(\\By^\\tr\\);\nFor integers \\(i&lt;j\\), you will see this shorthand all the time:\n\n\\[\ny_{i:j}\n=\n\\{y_i\\com y_{i+1}\\com y_{i+2}\\com...\\com y_{j-2}\\com y_{j-1}\\com y_{j}\\}\n.\n\\]\n\nThe symbol “\\(p\\)” will be aggressively abused and overloaded to represent any probability distribution, sometimes several in the same line:\n\n\\[\np(\\mathbf{y}_{0:T}) = p(\\mathbf{y}_0)\\prod_{t=1}^Tp(\\mathbf{y}_t\\,|\\,\\mathbf{y}_{0:t-1}).\n\\]"
  },
  {
    "objectID": "slides/ar-intro.html#the-simplest-non-trivial-time-series-model",
    "href": "slides/ar-intro.html#the-simplest-non-trivial-time-series-model",
    "title": "Autoregressive models",
    "section": "The simplest non-trivial time series model",
    "text": "The simplest non-trivial time series model\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n.\n\\end{aligned}\n\\]\n\n\nThat’s the recursive form. It implies a joint distribution written marginal-conditional style:\n\n\n\\[\n\\begin{aligned}\np(y_{1:T}\\mid y_0)\n&=\n\\prod_{t=1}^T\np(y_t\\given y_{0:t-1})\n\\\\\n&=\n\\prod_{t=1}^T\n\\underbrace{p(y_t\\given y_{t-1})}_{\\N(\\beta_0+\\beta_1y_{t-1}\\com\\sigma^2)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ar-intro.html#the-implied-joint-distribution",
    "href": "slides/ar-intro.html#the-implied-joint-distribution",
    "title": "Autoregressive models",
    "section": "The implied joint distribution",
    "text": "The implied joint distribution\nBecause the model is linear and Gaussian, the implied joint distribution across time is just a big ol’ multivariate normal:\n\\[\n\\begin{bmatrix}\ny_1 & y_2 & \\cdots & y_T\n\\end{bmatrix}^\\tr\\mid y_0\n\\sim\\text{N}_{T}\\left(\\Bmu\\com \\BSigma\\right).\n\\]\nJust like the linear mixed model is “just” a big multivariate normal with a particular block covariance structure (dependent within and independent between groups).\n\n\n\n\n\n\n\nDon’t worry about this!\n\n\nThe mean and covariance are given by:\n\\[\n\\begin{aligned}\nE(y_t\\mid y_0)\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^ty_0\n\\\\\n\\var(y_t\\mid y_0)\n&=\n\\sigma^2\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}\n\\\\\n\\cov(y_t\\com y_s\\mid y_0)\n&=\n\\begin{cases}\n\\beta_1^{s-t}\\var(y_t) & t\\leq s\\\\\n\\beta_1^{t-s}\\var(y_s) & s &lt; t.\n\\end{cases}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ar-intro.html#what-does-this-look-like",
    "href": "slides/ar-intro.html#what-does-this-look-like",
    "title": "Autoregressive models",
    "section": "What does this look like?",
    "text": "What does this look like?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nsimulate_ar_1 &lt;- function(T, b0, b1, s, m0, s0){\n  y &lt;- numeric(T)\n  y[1] &lt;- rnorm(1, m0, s0)\n  for(t in 2:T){\n    y[t] &lt;- b0 + b1 * y[t - 1] + rnorm(1, 0, s)\n  }\n  return(y)\n}\n\nar_1_mean &lt;- function(t, b0, b1, m0){\n  if(t == 0){\n    return(m0)\n  }else{\n    return(b0 * sum(b1 ^ (0:(t-1))) + m0 * (b1^t)) \n  }\n}\n\nar_1_var &lt;- function(t, b1, s, s0){\n  if(t == 0){\n    return(s0^2)\n  }else{\n    return((s0^2) * (b1^(2*t)) + (s^2) * sum(b1 ^ (2*(0:(t-1)))))\n  }\n}\n\nar_1_sd &lt;- function(t, b1, s, s0){\n  sqrt(ar_1_var(t, b1, s, s0))\n}\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"Marginal distributions and sample paths of a Gaussian AR(1)\"),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b0\",\n                  \"β₀\",\n                  min = -5,\n                  max = 5,\n                  value = 0,\n                  step = 0.1),\n      sliderInput(\"b1\",\n                  \"β₁\",\n                  min = -2,\n                  max = 2,\n                  value = 0,\n                  step = 0.1),\n      sliderInput(\"s\",\n                  \"σ\",\n                  min = 0,\n                  max = 2,\n                  value = 1, \n                  step = 0.1),\n      sliderInput(\"m0\",\n                  \"y₀\",\n                  min = -5,\n                  max = 5,\n                  value = 0,\n                  step = 0.1),\n      sliderInput(\"T\",\n                  \"T\",\n                  min = 20,\n                  max = 200,\n                  step = 20,\n                  value = 100),\n      actionButton(\"redo\", \"New sample path\"),\n    ),\n    \n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"distPlot\", height = \"600px\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  output$distPlot &lt;- renderPlot({\n    input$redo\n    b0 &lt;- input$b0\n    b1 &lt;- input$b1\n    redo &lt;- input$redo\n    T &lt;- input$T\n    s &lt;- input$s\n    m0 &lt;- input$m0\n    s0 = 0\n    \n    range = 0:T\n    alpha = c(0.01, seq(0.1, 0.9, by = 0.1))\n    \n    middle &lt;- sapply(range, ar_1_mean, b0, b1, m0)\n    sds &lt;- sapply(range, ar_1_sd, b1, s, s0)\n    \n    \n    plot(range, middle, type = \"l\",\n         xaxt = \"n\", \n         yaxt = \"n\",\n         xlab = \"t\",\n         ylab = expression(y[t]),\n         ylim = c(-20, 20), bty = \"n\",\n         col = \"white\")\n    \n    for(a in alpha){\n      \n      U = qnorm(1 - a / 2, mean = middle, sd = sds)\n      L = qnorm(a / 2, mean = middle, sd = sds)\n      \n      polygon(\n        c(range, rev(range)),\n        c(U, rev(L)),\n        col = rgb(1, 0, 0, 0.15),\n        border = NA\n      )\n    }\n    \n    inc = 20\n    axis(1, pos = 0, at = seq(0, max(range), by = inc), \n         labels = c(NA, seq(inc, max(range), by = inc)))\n    axis(2, pos = 0)\n    \n    lines(range, simulate_ar_1(max(range) + 1, b0, b1, s, m0, s0), col = \"black\", lwd = 2)\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "slides/ar-intro.html#observations",
    "href": "slides/ar-intro.html#observations",
    "title": "Autoregressive models",
    "section": "Observations",
    "text": "Observations\n\n\\(|\\beta_1|&lt;1\\): stable;\n\\(|\\beta_1|&gt;1\\): explosive;\n\\(\\beta_1&lt;0\\): oscillation;\n\\(|\\beta_1|=1\\): random walk (w/ drift if \\(\\beta_0\\neq0\\))."
  },
  {
    "objectID": "slides/ar-intro.html#what-do-we-mean-by-stable",
    "href": "slides/ar-intro.html#what-do-we-mean-by-stable",
    "title": "Autoregressive models",
    "section": "What do we mean by stable?",
    "text": "What do we mean by stable?\n\nA joint distribution is (strictly) stationary if it is “shift invariant”:\n\n\\[\n\\{y_{t_1}\\com y_{t_2}\\com ...\\com y_{t_n}\\}\\overset{d}{=}\\{y_{t_1+h}\\com y_{t_2+h}\\com ...\\com y_{t_n+h}\\}.\n\\]\n\nThe joint distribution is the same anywhere you choose to zoom in;\nThe Gaussian AR(1) with \\(|\\beta_1|&lt;1\\) has this property."
  },
  {
    "objectID": "slides/ar-intro.html#stationary-ar1",
    "href": "slides/ar-intro.html#stationary-ar1",
    "title": "Autoregressive models",
    "section": "Stationary AR(1)",
    "text": "Stationary AR(1)\nIf \\(-1&lt;\\beta_1&lt;1\\) and \\(y_0=\\beta_0/(1-\\beta_1)\\), then the AR(1) is strictly stationary with the following:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\frac{\\beta_0}{1-\\beta_1}\n\\\\\n\\var(y_t)\n&=\n\\frac{\\sigma^2}{1-\\beta_1^2}\n\\\\\n\\cov(y_t\\com y_s)\n&=\n\\beta_1^{|t-s|}\\var(y_t)\n=\n\\beta_1^{|t-s|}\\frac{\\sigma^2}{1-\\beta_1^2}.\n\\end{aligned}\n\\]\nThe common marginal shared by all \\(y_t\\) is called the stationary distribution:\n\\[\ny_t\\sim\\text{N}\\left(\\frac{\\beta_0}{1-\\beta_1}\\com \\frac{\\sigma^2}{1-\\beta_1^2}\\right).\n\\]\nSo “did: dependent but identically distributed.”"
  },
  {
    "objectID": "slides/ar-intro.html#autocovariance-of-a-stationary-process",
    "href": "slides/ar-intro.html#autocovariance-of-a-stationary-process",
    "title": "Autoregressive models",
    "section": "Autocovariance of a stationary process",
    "text": "Autocovariance of a stationary process\nFor a stationary process, the covariance kernel satisfies\n\\[\n\\cov(y_t\\com y_{s})=\\cov(y_{t+h}\\com y_{s+h})\\quad \\forall (t\\com s\\com h).\n\\]\nSo you can define something called the autocovariance function:\n\\[\n\\gamma(h)=\\cov(y_{t+h}\\com y_{t}).\n\\]\nFor the AR(1), this is\n\\[\n\\begin{aligned}\n\\gamma(0)&=\\sigma^2/(1-\\beta_1^2)\n\\\\\n\\gamma(h)&=\\beta_1^h\\gamma(0).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ar-intro.html#what-does-the-autocov-function-look-like",
    "href": "slides/ar-intro.html#what-does-the-autocov-function-look-like",
    "title": "Autoregressive models",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"Theoretical autocovariance of a stationary AR(1)\"),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b1\",\n                  \"β₁\",\n                  min = -0.99,\n                  max = 0.99,\n                  value = 0,\n                  step = 0.01),\n      sliderInput(\"s\",\n                  \"σ\",\n                  min = 0,\n                  max = 2,\n                  value = 1, \n                  step = 0.1),\n      sliderInput(\"H\",\n                  \"H\",\n                  min = 0,\n                  max = 50,\n                  step = 1,\n                  value = 20)\n    ),\n    \n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"distPlot\", height = \"500px\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  output$distPlot &lt;- renderPlot({\n    beta_1 &lt;- input$b1\n    max_lag &lt;- input$H\n    sigma2 &lt;- input$s^2\n\n    # Compute theoretical autocovariances\n    lags &lt;- 0:max_lag\n    gamma_h &lt;- sigma2 / (1 - beta_1^2) * beta_1^lags\n    \n    # Plot\n    plot(lags, gamma_h, type=\"h\", lwd=2,\n         xlab=\"Lag h\", ylab=\"γ(h)\",\n         main=\"\")\n    points(lags, gamma_h, pch=19, col=\"blue\")\n    abline(h = 0, lty = 2, col = \"darkgrey\")\n    \n    # Add legend using Unicode\n    legend_text &lt;- paste0(\"β1 = \", beta_1, \", σ² = \", sigma2)\n    legend(\"topright\", legend=legend_text, bty=\"n\")\n\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "slides/ar-intro.html#observations-1",
    "href": "slides/ar-intro.html#observations-1",
    "title": "Autoregressive models",
    "section": "Observations",
    "text": "Observations\nThe autocovariance of a stationary AR(1) is:\n\\[\n\\begin{aligned}\n\\gamma(0)&=\\sigma^2/(1-\\beta_1^2)\n\\\\\n\\gamma(h)&=\\beta_1^h\\gamma(0).\n\\end{aligned}\n\\]\n\n\\(\\gamma(h)\\to0\\) as \\(h\\to\\infty\\);\nThe larger \\(|\\beta_1|\\), the stronger the serial dependence, the slower the decay;\nIf \\(-1&lt;\\beta_1&lt;0\\), you get oscillation because \\(\\beta_1^h\\) is positive for even \\(h\\) and negative for odd \\(h\\)."
  },
  {
    "objectID": "slides/ar-intro.html#theres-nothing-special-about-one-lag",
    "href": "slides/ar-intro.html#theres-nothing-special-about-one-lag",
    "title": "Autoregressive models",
    "section": "There’s nothing special about one lag",
    "text": "There’s nothing special about one lag\nThe autoregression of order \\(p\\), or AR(\\(p\\)):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\beta_2\ny_{t-2}\n+\n\\cdots\n+\n\\beta_p\ny_{t-p}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\end{aligned}\n\\]\nThis again implies a joint distribution written marginal-conditional style:\n\\[\n\\begin{aligned}\np(y_{1:T}\\mid y_0\\com y_{-1}\\com ...\\com y_{1-p})\n&=\n\\prod_{t=1}^Tp(y_t\\mid y_{1-p:t-1})\\\\\n\\\\\n&=\n\\prod_{t=1}^Tp(y_t\\mid y_{t-p:t-1}).\n\\end{aligned}\n\\]\nAs before, the joint distribution is just a big ol’ multivariate normal, but the mean and covariance are harder to characterize."
  },
  {
    "objectID": "slides/ar-intro.html#lets-improve-the-notation",
    "href": "slides/ar-intro.html#lets-improve-the-notation",
    "title": "Autoregressive models",
    "section": "Let’s improve the notation",
    "text": "Let’s improve the notation\nThe autoregression of order p, or AR(p):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\sum\\limits_{\\ell=1}^p\n\\beta_\\ell\ny_{t-\\ell}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2).\n\\end{aligned}\n\\]\n\n\nThis implies a joint distribution governed by a finite set of static parameters \\(\\Btheta = \\begin{bmatrix}\\beta_0&\\beta_1&\\cdots &\\beta_p&\\sigma^2\\end{bmatrix}^\\tr\\):\n\n\n\\[\n\\begin{aligned}\np(y_{1:T}\\given y_{1-p:0}\\com\\Btheta)\n&=\n\\prod_{t=1}^T\np(y_t\\given y_{t-p:t-1}\\com\\Btheta).\n\\end{aligned}\n\\]\n\n\nViewed as a function of \\(\\Btheta\\), that’s a (conditional) likelihood!"
  },
  {
    "objectID": "slides/ar-intro.html#maximum-likelihood-estimation",
    "href": "slides/ar-intro.html#maximum-likelihood-estimation",
    "title": "Autoregressive models",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nTreating the observed data \\(y_{1-p:T}\\) as fixed, we want:\n\\[\n\\hat{\\Btheta}_T=\\argmax{\\Btheta}\\,p(y_{1:T}\\given y_{1-p:0}\\com \\Btheta).\n\\]\nTo do this, it helps to view the AR(p) is “just” a multiple linear regression where \\(\\Bx_t\\) encodes the lagged values of \\(y_t\\):\n\\[\n\\begin{aligned}\ny_t&=\\Bx_t^\\tr\\Bbeta+\\varepsilon_t,&&\\varepsilon_t\\iid\\N(0\\com\\sigma^2)\n\\\\\n\\Bx_t&=\\begin{bmatrix}\n1 & y_{t-1} & y_{t-2} & \\cdots & y_{t-p}\n\\end{bmatrix}^\\tr\\\\\n\\Bbeta&=\\begin{bmatrix}\n\\beta_0 & \\beta_1 & \\beta_2 & \\cdots & \\beta_p\n\\end{bmatrix}^\\tr.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ar-intro.html#stack-em-up",
    "href": "slides/ar-intro.html#stack-em-up",
    "title": "Autoregressive models",
    "section": "Stack ’em up",
    "text": "Stack ’em up\nConstruct the response vector and design matrix:\n\n\\[\n\\begin{aligned}\n\\underbrace{\\By_T}_{T\\times 1}\n&=\n\\begin{bmatrix}y_1&y_2 & \\cdots & y_T\\end{bmatrix}^\\tr\n\\\\\n\\underbrace{\\BX_T}_{T\\times (p+1)}\n&=\n\\begin{bmatrix}\n1 & y_0 & y_{-1} & \\cdots & y_{1-p} \\\\\n1 & y_1 & y_0 & \\cdots & y_{2-p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & y_{T-1} & y_{T-2} & \\cdots & y_{T-p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\Bx_1^\\tr\\\\\n\\Bx_2^\\tr\\\\\n\\vdots\\\\\n\\Bx_T^\\tr\n\\end{bmatrix}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ar-intro.html#ols-for-the-arp",
    "href": "slides/ar-intro.html#ols-for-the-arp",
    "title": "Autoregressive models",
    "section": "OLS for the AR(p)",
    "text": "OLS for the AR(p)\n\nTurns out, the maximum (conditional) likelihood estimator in the AR(p) is the same as the ordinary least squares estimator:\n\n\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_T\n&=\n(\\BX_T^\\tr\\BX_T)^{-1}\\BX_T^\\tr\\By_T\n\\\\\n\\hat{\\sigma^2_T}\n&=\n||\\By_T-\\BX_T\\hat{\\Bbeta}_T||_2^2 / T.\n\\end{aligned}\n\\]\n\n\n\nIt doesn’t matter that there’s time series dependence. Once the data are observed and fixed, the mechanics of computing the estimate are identical to iid multiple regression;\nThe analogy breaks down after that: different sampling distributions, intervals, etc. No exact procedures. Need bootstrap, etc."
  },
  {
    "objectID": "slides/ar-intro.html#bayes-is-just-like-iid-regression-too",
    "href": "slides/ar-intro.html#bayes-is-just-like-iid-regression-too",
    "title": "Autoregressive models",
    "section": "Bayes is just like iid regression too",
    "text": "Bayes is just like iid regression too\nBayesian model with a conjugate prior:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0\\com b_0)\n\\\\\n\\Bbeta\\given \\sigma^2\n&\\sim\n\\text{N}_{p+1}(\\Bm_0\\com\\sigma^2\\BH^{-1}_0)\n\\\\\ny_t\n\\given\n\\Bx_t\n\\com\n\\Bbeta\\com\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\Bx_t^\\tr\\Bbeta\\com\\sigma^2\n\\right).\n\\end{aligned}\n\\]\n\nThe posterior is available in closed-form:\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:T}\n&\\sim\n\\text{IG}(a_T\\com b_T)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:T}\n&\\sim\n\\text{N}_{p+2}(\\Bm_T\\com\\sigma^2\\BH^{-1}_T)\n\\\\\n\\\\\n\\BH_T\n&=\n\\BX_T^\\tr\\BX_T+\\BH_0\n\\\\\n\\Bm_T\n&=\n\\BH_T^{-1}(\\BX_T^\\tr\\By_T+\\BH_0\\Bm_0)\n\\\\\na_T\n&=\na_0 + T/2\n\\\\\nb_T\n&=\nb_0\n+\n(\\By_T^\\tr\\By_T+\\Bm_0^\\tr\\BH_0\\Bm_0-\\Bm_T^\\tr\\BH_T\\Bm_T)/2.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ar-intro.html#lag-order-selection",
    "href": "slides/ar-intro.html#lag-order-selection",
    "title": "Autoregressive models",
    "section": "Lag order selection",
    "text": "Lag order selection\nThe lag order \\(p\\) is a hyperparameter that needs to be selected. \\(p\\) determines the number of predictors in the (auto)regression model, and so selecting the lag order is analogous to variable selection in iid regression."
  },
  {
    "objectID": "slides/ar-intro.html#rules-of-thumb",
    "href": "slides/ar-intro.html#rules-of-thumb",
    "title": "Autoregressive models",
    "section": "Rules of thumb",
    "text": "Rules of thumb\nPick \\(p\\) = number of observations in one natural cycle of the data:\n\n\n\n\nData frequency\nTypical lag length (p)\n\n\n\n\nHourly\n24\n\n\nDaily\n7\n\n\nMonthly\n12\n\n\nQuarterly\n4\n\n\nAnnual\n1\n\n\n\n\n\nFine as a quick-and-dirty solution. Guidance on this will be domain-specific."
  },
  {
    "objectID": "slides/ar-intro.html#minimize-some-criterion",
    "href": "slides/ar-intro.html#minimize-some-criterion",
    "title": "Autoregressive models",
    "section": "Minimize some criterion",
    "text": "Minimize some criterion\nTake:\n\\[\n\\hat{p}=\\argmin{p=1\\com2\\com...\\com\\bar{p}}\\,\\text{C}(p;\\,\\By_T\\com\\BX_T).\n\\]\n\nCommon choices:\n\nAkaike information criterion (AIC);\nBayesian information criterion (BIC);\nCross-validation error;\n\n\n\nCV assesses out-of-sample performance and is highly customizable, but can be more labor-intensive to implement."
  },
  {
    "objectID": "slides/ar-intro.html#recall-penalized-iid-regression",
    "href": "slides/ar-intro.html#recall-penalized-iid-regression",
    "title": "Autoregressive models",
    "section": "Recall: penalized iid regression",
    "text": "Recall: penalized iid regression\nPenalized estimator for the linear model:\n\\[\n\\hat{\\Bbeta}_{\\lambda}\n=\n\\underset{\\Bbeta}{\\arg\\min}\\left\\{||\\By-\\BX\\Bbeta||_2^2+\\lambda\\mathcal{P}(\\Bbeta)\\right\\}.\n\\] The penalty provides shrinkage or regularization, and the tuning parameter \\(\\lambda&gt;0\\) governs how much:\n\nRidge regression: \\(\\mathcal{P}(\\Bbeta)=||\\Bbeta||_2^2\\);\nLASSO: \\(\\mathcal{P}(\\Bbeta)=||\\Bbeta||_1\\);\netc.\n\nHow do you tune \\(\\lambda\\)?"
  },
  {
    "objectID": "slides/ar-intro.html#recall-loo-cv-for-iid-regression",
    "href": "slides/ar-intro.html#recall-loo-cv-for-iid-regression",
    "title": "Autoregressive models",
    "section": "Recall: LOO-CV for iid regression",
    "text": "Recall: LOO-CV for iid regression\nLeave-one-out cross-validation:\n\n\nFor each model configuration, train on all data except observation \\(i\\), and then test on held-out \\((\\Bx_i\\com y_i)\\);\n\n\n\n\nAverage prediction error over all training/test splits, and pick the best model:\n\n\n\n\\[\n\\hat{\\lambda}=\\argmin{\\lambda\\geq0}\\,\\sum\\limits_{i=1}^n\\left(y_i-\\Bx_i^\\tr\\hat{\\Bbeta}_\\lambda^{(-i)}\\right)^2\n.\n\\]\n\n\n\nOptimizes out-of-sample point prediction accuracy;\n\n\n\n\nFine for iid, but with serially dependent time series data, it’s not appropriate to randomly rip observations out of the middle and fit a model to what’s left before and after. Gotta respect the time-ordering."
  },
  {
    "objectID": "slides/ar-intro.html#time-series-cross-validation",
    "href": "slides/ar-intro.html#time-series-cross-validation",
    "title": "Autoregressive models",
    "section": "Time series cross-validation",
    "text": "Time series cross-validation\nLeave-future-out cross-validation (LFO-CV):\n\\[\n\\begin{aligned}\n&{\\color{blue}\\bullet\\,\\text{training}}\\quad{\\color{red}\\bullet\\,\\text{test}}\\\\\n&\\begin{matrix}\n\\color{blue}{y_1} & \\color{lightgray}{y_2} & \\color{lightgray}{y_3} & \\color{red}{y_4} & \\color{lightgray}{y_5} & \\color{lightgray}{y_6} & \\color{lightgray}{y_7} & \\color{lightgray}{y_8} & \\color{lightgray}{y_9} & \\color{lightgray}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_1^{(p)} \\\\\n\\color{blue}{y_1} & \\color{blue}{y_2} & \\color{lightgray}{y_3} & \\color{lightgray}{y_4} & \\color{red}{y_5} & \\color{lightgray}{y_6} & \\color{lightgray}{y_7} & \\color{lightgray}{y_8} & \\color{lightgray}{y_9} & \\color{lightgray}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_2^{(p)} \\\\\n\\color{blue}{y_1} & \\color{blue}{y_2} & \\color{blue}{y_3} & \\color{lightgray}{y_4} & \\color{lightgray}{y_5} & \\color{red}{y_6} & \\color{lightgray}{y_7} & \\color{lightgray}{y_8} & \\color{lightgray}{y_9} & \\color{lightgray}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_3^{(p)} \\\\\n\\color{blue}{y_1} & \\color{blue}{y_2} & \\color{blue}{y_3} & \\color{blue}{y_4} & \\color{lightgray}{y_5} & \\color{lightgray}{y_6} & \\color{red}{y_7} & \\color{lightgray}{y_8} & \\color{lightgray}{y_9} & \\color{lightgray}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_4^{(p)} \\\\\n\\color{blue}{y_1} & \\color{blue}{y_2} & \\color{blue}{y_3} & \\color{blue}{y_4} & \\color{blue}{y_5} & \\color{lightgray}{y_6} & \\color{lightgray}{y_7} & \\color{red}{y_8} & \\color{lightgray}{y_9} & \\color{lightgray}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_5^{(p)} \\\\\n\\color{blue}{y_1} & \\color{blue}{y_2} & \\color{blue}{y_3} & \\color{blue}{y_4} & \\color{blue}{y_5} & \\color{blue}{y_6} & \\color{lightgray}{y_7} & \\color{lightgray}{y_8} & \\color{red}{y_9} & \\color{lightgray}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_6^{(p)} \\\\\n\\color{blue}{y_1} & \\color{blue}{y_2} & \\color{blue}{y_3} & \\color{blue}{y_4} & \\color{blue}{y_5} & \\color{blue}{y_6} & \\color{blue}{y_7} & \\color{lightgray}{y_8} & \\color{lightgray}{y_9} & \\color{red}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_7^{(p)}\\\\\n&&&&&\\vdots&&&&&&&\\vdots\\\\\n\\end{matrix}\n\\end{aligned}\n\\]\n\nThen pick:\n\\[\n\\hat{p}=\\argmin{p=1\\com2\\com...\\com\\bar{p}}\\,\\sum\\limits_{t=1}^T\\text{error}_t^{(p)}\n\\]"
  },
  {
    "objectID": "slides/ar-intro.html#cross-validation",
    "href": "slides/ar-intro.html#cross-validation",
    "title": "Autoregressive models",
    "section": "Cross-validation",
    "text": "Cross-validation\nVirtues:\n\nout-of-sample: we care most about how methods perform on data we haven’t seen yet. CV mimics that directly;\ncustomizable: whatever prediction task you care about (8-step-ahead point prediction, interval prediction, density, etc), simply plug-in the error metric that matches;\nmethod-agnostic: once we have the predictions, it doesn’t matter where they came from. CV provides an apples-to-apples comparison across wildly differing procedures;\n\nVices:\nHigh computational overhead to actually implement. Ideally, you fully re-train the entire procedure on every possible split. This can be very costly. So then what? Parallelize over splits? Only partially re-train? Randomly sample a subset of the splits and accept a noisier estimate of the CV error?\nSacrifices will have to be made somewhere."
  },
  {
    "objectID": "slides/ar-forecast.html#the-simplest-non-trivial-time-series-model",
    "href": "slides/ar-forecast.html#the-simplest-non-trivial-time-series-model",
    "title": "Probabilistic predictions",
    "section": "The simplest non-trivial time series model",
    "text": "The simplest non-trivial time series model\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2).\n\\end{aligned}\n\\]\nThis implies a joint distribution (it’s multivariate normal!) governed by a finite set of static parameters \\(\\Btheta = \\begin{bmatrix}\\beta_0&\\beta_1&\\sigma^2\\end{bmatrix}^\\tr\\):\n\\[\n\\begin{aligned}\np(y_{1:T}\\given y_0\\com \\Btheta)\n&=\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}\\com\\Btheta).\n\\end{aligned}\n\\]\nViewed as a function of \\(\\Btheta\\), that’s a (conditional) likelihood!"
  },
  {
    "objectID": "slides/ar-forecast.html#likelihood-based-inference",
    "href": "slides/ar-forecast.html#likelihood-based-inference",
    "title": "Probabilistic predictions",
    "section": "Likelihood-based inference",
    "text": "Likelihood-based inference\nClassical route:\n\\[\n\\hat{\\Btheta}_T=\\argmax{\\Btheta}\\,p(y_{1:T}\\given y_0\\com \\Btheta).\n\\]\nBayesian route:\n\\[\np(\\Btheta\\given y_{0:T})\n=\n\\frac{p(y_{1:T}\\given y_0\\com \\Btheta)p(\\Btheta)}{p(y_{1:T}\\given y_0)}\n.\n\\]\nThe raw calculations are similar to iid regression, but the inferential theory can be very different because the \\(y_t\\) are now dependent."
  },
  {
    "objectID": "slides/ar-forecast.html#maximum-likelihood",
    "href": "slides/ar-forecast.html#maximum-likelihood",
    "title": "Probabilistic predictions",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nIt’s “just” ordinary least squares (OLS):\n\n\\[\n\\begin{aligned}\n\\By_T\n&=\n\\begin{bmatrix}y_1&y_2 & \\cdots & y_T\\end{bmatrix}^\\tr\n\\\\\n\\BX_T\n&=\n\\begin{bmatrix}\n1 & 1 & \\cdots & 1 \\\\\ny_0 & y_1 & \\cdots & y_{T-1}\n\\end{bmatrix}^\\tr\n\\\\\n\\Bbeta\n&=\n\\begin{bmatrix}\\beta_0&\\beta_1\\end{bmatrix}^\\tr\n\\\\\n\\\\\n\\hat{\\Bbeta}_T\n&=\n(\\BX_T^\\tr\\BX_T)^{-1}\\BX_T^\\tr\\By_T\n\\\\\n\\hat{\\sigma^2_T}\n&=\n||\\By_T-\\BX_T\\hat{\\Bbeta}_T||_2^2 / T.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ar-forecast.html#conjugate-bayes",
    "href": "slides/ar-forecast.html#conjugate-bayes",
    "title": "Probabilistic predictions",
    "section": "Conjugate Bayes",
    "text": "Conjugate Bayes\nTake a conjugate normal-inverse-gamma prior:\n\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0\\com b_0)\n\\\\\n\\Bbeta\\given \\sigma^2\n&\\sim\n\\text{N}_2(\\Bm_0\\com\\sigma^2\\BH^{-1}_0)\n\\\\\ny_t\n\\given\ny_{t-1}\n\\com\n\\Bbeta\\com\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\Bx_t^\\tr\\Bbeta\\com\\sigma^2\n\\right), && \\Bx_t=\\begin{bmatrix}1 & y_{t-1}\\end{bmatrix}^\\tr.\n\\end{aligned}\n\\]\n\n\nThe posterior is available in closed-form:\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:T}\n&\\sim\n\\text{IG}(a_T\\com b_T)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:T}\n&\\sim\n\\text{N}_2(\\Bm_T\\com\\sigma^2\\BH^{-1}_T)\n\\\\\n\\\\\n\\BH_T\n&=\n\\BX_T^\\tr\\BX_T+\\BH_0\n\\\\\n\\Bm_T\n&=\n\\BH_T^{-1}(\\BX_T^\\tr\\By_T+\\BH_0\\Bm_0)\n\\\\\na_T\n&=\na_0 + T/2\n\\\\\nb_T\n&=\nb_0\n+\n(\\By_T^\\tr\\By_T+\\Bm_0^\\tr\\BH_0\\Bm_0-\\Bm_T^\\tr\\BH_T\\Bm_T)/2.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ar-forecast.html#lets-think-about-probabilistic-prediction",
    "href": "slides/ar-forecast.html#lets-think-about-probabilistic-prediction",
    "title": "Probabilistic predictions",
    "section": "Let’s think about probabilistic prediction",
    "text": "Let’s think about probabilistic prediction"
  },
  {
    "objectID": "slides/ar-forecast.html#point-forecast",
    "href": "slides/ar-forecast.html#point-forecast",
    "title": "Probabilistic predictions",
    "section": "Point forecast",
    "text": "Point forecast\nYour single-number best guess at tomorrow’s observation:"
  },
  {
    "objectID": "slides/ar-forecast.html#forecast-interval",
    "href": "slides/ar-forecast.html#forecast-interval",
    "title": "Probabilistic predictions",
    "section": "Forecast interval",
    "text": "Forecast interval\nA range of likely values for tomorrow’s observation:"
  },
  {
    "objectID": "slides/ar-forecast.html#forecast-density",
    "href": "slides/ar-forecast.html#forecast-density",
    "title": "Probabilistic predictions",
    "section": "Forecast density",
    "text": "Forecast density\nFull distribution capturing uncertainty about tomorrow:"
  },
  {
    "objectID": "slides/ar-forecast.html#and-then-tomorrow-finally-comes",
    "href": "slides/ar-forecast.html#and-then-tomorrow-finally-comes",
    "title": "Probabilistic predictions",
    "section": "And then tomorrow finally comes",
    "text": "And then tomorrow finally comes\nSo…how’d we do?"
  },
  {
    "objectID": "slides/ar-forecast.html#whats-the-point",
    "href": "slides/ar-forecast.html#whats-the-point",
    "title": "Probabilistic predictions",
    "section": "What’s the point?",
    "text": "What’s the point?\n\nWe want intervals and densities to communicate uncertainty about the forecast;\n\nWhat sources of uncertainty?\n\nData uncertainty (data are realization of random process);\nParameter estimation uncertainty;\nHyperparameter tuning uncertainty;\nModel uncertainty;\nUncertainty introduced by missing data;\n\n\nIn the small world of the AR(1), mainly the first two for now."
  },
  {
    "objectID": "slides/ar-forecast.html#forecast-distribution",
    "href": "slides/ar-forecast.html#forecast-distribution",
    "title": "Probabilistic predictions",
    "section": "Forecast distribution",
    "text": "Forecast distribution\n\n\nGiven the data \\(y_{0:t}\\) we’ve seen, we want a full distribution for a future observation \\(y_{t+h}\\) we haven’t seen yet;\nIn other words, we want a conditional distribution for the future given the past;\nWe will momentarily treat the point estimate \\(\\hat{\\Btheta} = \\begin{bmatrix}\\hat{\\beta}_0&\\hat{\\beta}_1&\\widehat{\\sigma^2}\\end{bmatrix}^\\tr\\) as if it were the fixed and known truth (newsflash: it isn’t!), and we will forecast using\n\n\\[\np(y_{t+h}\\mid y_{0:t}\\com \\hat{\\Btheta}).\n\\]\nSo, what is that conditional distribution?"
  },
  {
    "objectID": "slides/ar-forecast.html#its-a-normal",
    "href": "slides/ar-forecast.html#its-a-normal",
    "title": "Probabilistic predictions",
    "section": "It’s a normal!",
    "text": "It’s a normal!\nWriting down an AR(1) is “just” an alternative way of writing down a big multivariate normal across all time:\n\\[\n\\begin{bmatrix}\ny_1 & y_2 & \\cdots & y_t & y_{t+1} & y_{t+2} & \\cdots & y_{t+H}\n\\end{bmatrix}^\\tr\\mid y_0\\com \\hat{\\Btheta}\n\\sim\\text{N}_{T}\\left(\\Bmu(\\hat{\\Btheta})\\com \\BSigma(\\hat{\\Btheta})\\right).\n\\]\nAll of the marginals and conditionals of the multivariate normal are normal, and so however far into the future you want to go, you get:\n\\[\ny_{t+h} \\mid y_{0:t}\\com \\hat{\\Btheta}\\sim\\N\\left(\\mu_{t+h|t}\\com \\sigma^2_{t+h|t}\\right).\n\\]\nLet the computer deal with the means and variances."
  },
  {
    "objectID": "slides/ar-forecast.html#prediction-intervals",
    "href": "slides/ar-forecast.html#prediction-intervals",
    "title": "Probabilistic predictions",
    "section": "Prediction intervals",
    "text": "Prediction intervals\nThe full predictive distribution is\n\\[\ny_{t+h} \\mid y_{0:t}\\com \\hat{\\Btheta}\\sim\\N\\left(\\mu_{t+h|t}\\com \\sigma^2_{t+h|t}\\right),\n\\]\nThe point prediction is \\(\\mu_{t+h|t}\\), and the prediction interval has the usual form:\n\\[\n\\begin{aligned}\n\\mu_{t+h|t}\n&\\pm\nz_{1-\\frac{\\alpha}{2}}^\\star\n\\sigma_{t+h|t}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ar-forecast.html#pics-or-it-didnt-happen",
    "href": "slides/ar-forecast.html#pics-or-it-didnt-happen",
    "title": "Probabilistic predictions",
    "section": "Pics or it didn’t happen",
    "text": "Pics or it didn’t happen\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\n\nsimulate_ar_1 &lt;- function(T, b0, b1, s, y0){\n  y &lt;- numeric(T)\n  y[1] &lt;- y0\n  for(t in 2:T){\n    y[t] &lt;- b0 + b1 * y[t - 1] + rnorm(1, 0, s)\n  }\n  return(y)\n}\n\nar_1_mean &lt;- function(h, b0, b1, yT){\n  if(h == 0){\n    return(yT)\n  } else {\n    return(b0 * sum(b1 ^ (0:(h-1))) + yT * (b1^h)) \n  }\n}\n\nar_1_var &lt;- function(h, b1, s){\n  if(h == 0){\n    return(0)\n  } else {\n    return((s^2) * sum(b1 ^ (2*(0:(h-1)))))\n  }\n}\n\nui &lt;- fluidPage(\n  titlePanel(\"Forecast distribution of a Gaussian AR(1) with known parameters\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b0\", \"β₀\", min = -5, max = 5, value = 0, step = 0.1),\n      sliderInput(\"b1\", \"β₁\", min = -2, max = 2, value = 0, step = 0.1),\n      sliderInput(\"s\", \"σ\", min = 0, max = 2, value = 1, step = 0.1)\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  # fixed observed data\n  set.seed(123)\n  y_obs &lt;- simulate_ar_1(10, 0, 0, 1, 0)\n  \n  output$distPlot &lt;- renderPlot({\n    b0 &lt;- input$b0\n    b1 &lt;- input$b1\n    s &lt;- input$s\n    \n    T_obs &lt;- length(y_obs)\n    H &lt;- 20  # forecast horizon\n    range &lt;- 1:(T_obs + H)\n    \n    # plot window\n    plot(range, c(y_obs, rep(NA,H)), type = \"n\",\n         xlab = \"t\", ylab = expression(y[t]),\n         ylim = c(-20,20), bty=\"n\", main = \"This is called a fan chart.\")\n    \n    # grey forecast region\n    rect(T_obs+0.5, -20, T_obs + H + 0.5, 20, col = rgb(0.8,0.8,0.8,0.5), border = NA)\n    \n    # observed data\n    lines(1:T_obs, y_obs, col=\"black\", lwd=2)\n    \n    # forecast distribution intervals\n    alpha = c(0.01, seq(0.1,0.9,0.1))\n    middle &lt;- sapply(0:H, ar_1_mean, b0, b1, y_obs[T_obs])\n    sds &lt;- sqrt(sapply(0:H, ar_1_var, b1, s))\n    f_range &lt;- T_obs:(T_obs+H)\n    \n    for(a in alpha){\n      U = qnorm(1 - a/2, mean = middle, sd = sds)\n      L = qnorm(a/2, mean = middle, sd = sds)\n      polygon(c(f_range, rev(f_range)),\n              c(U, rev(L)),\n              col = rgb(1,0,0,0.15), border=NA)\n    }\n    \n    # add mean forecast line\n    lines(f_range, middle, col=\"red\", lwd=2, lty=2)\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "slides/ar-forecast.html#but-wait",
    "href": "slides/ar-forecast.html#but-wait",
    "title": "Probabilistic predictions",
    "section": "But wait!",
    "text": "But wait!\n\nThe parameters aren’t known. They’re a noisy estimate computed from imperfect data;\nThere is sampling uncertainty associated with \\(\\hat{\\Btheta}\\). Shouldn’t that extra source of uncertainty be propagated through to the predictive distribution?\nYes, but how? Can we just plug in estimates and replace \\(z\\) quantiles with \\(t\\) quantiles?\nNot quite. That was fine for iid regression, but it all breaks under time series dependence."
  },
  {
    "objectID": "slides/ar-forecast.html#bootstrap",
    "href": "slides/ar-forecast.html#bootstrap",
    "title": "Probabilistic predictions",
    "section": "Bootstrap!",
    "text": "Bootstrap!\nTo produce prediction intervals that incorporate both future data uncertainty and parameter estimation uncertainty, you need to use the bootstrap. But again, it’s not the vanilla, iid bootstrap. It’s a bootstrap on the residuals, which are assumed iid in this model.\nThe details are in an appendix at the end of this deck."
  },
  {
    "objectID": "slides/ar-forecast.html#compare-intervals",
    "href": "slides/ar-forecast.html#compare-intervals",
    "title": "Probabilistic predictions",
    "section": "Compare intervals",
    "text": "Compare intervals\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"AR(1) Forecast: Plug-in vs Residual Bootstrap\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"true_b0\", \"True β₀:\", min = -1, max = 1, value = 0.5, step = 0.1),\n      sliderInput(\"true_b1\", \"True β₁:\", min = -0.95, max = 0.95, value = 0.7, step = 0.05),\n      sliderInput(\"true_sigma\", \"True σ:\", min = 0.1, max = 2, value = 1, step = 0.1),\n      sliderInput(\"n_obs\", \"Sample size:\", min = 40, max = 500, value = 100, step = 10),\n      actionButton(\"rerun\", \"Re-run simulation\"),\n      checkboxInput(\"show_red\", \"Show plug-in fan (red)\", TRUE),\n      checkboxInput(\"show_blue\", \"Show bootstrap fan (blue)\", TRUE)\n    ),\n    \n    mainPanel(\n      plotOutput(\"fanPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  simulate_ar_1 &lt;- function(T, b0, b1, s, y1 = 0){\n    y &lt;- numeric(T)\n    y[1] &lt;- y1\n    for(t in 2:T){\n      y[t] &lt;- b0 + b1 * y[t-1] + rnorm(1, 0, s)\n    }\n    return(y)\n  }\n  \n  ar_1_mean_h &lt;- function(h, b0, b1, yT){\n    if(h == 0) return(yT)\n    b0 * sum(b1^(0:(h-1))) + yT * (b1^h)\n  }\n  \n  ar_1_var_h &lt;- function(h, b1, sigma){\n    if(h == 0) return(0)\n    sigma^2 * sum(b1^(2*(0:(h-1))))\n  }\n  \n  sim_data &lt;- reactiveVal(NULL)\n  \n  observeEvent(input$rerun, {\n    set.seed(123)  # keep deterministic for reproducibility\n    n_obs &lt;- input$n_obs\n    H &lt;- 20\n    B &lt;- 7500\n    \n    y_obs &lt;- simulate_ar_1(n_obs, input$true_b0, input$true_b1, input$true_sigma, y1 = 0)\n    \n    # OLS fit\n    Y &lt;- y_obs[2:n_obs]\n    X &lt;- cbind(1, y_obs[1:(n_obs-1)])\n    ols_fit &lt;- lm(Y ~ X - 1)\n    coef_hat &lt;- coef(ols_fit)\n    b0_hat &lt;- coef_hat[1]\n    b1_hat &lt;- coef_hat[2]\n    resid_hat &lt;- resid(ols_fit)\n    sigma_hat &lt;- sqrt(sum(resid_hat^2) / (length(resid_hat) - 1))\n    \n    h_seq &lt;- 0:H\n    plug_mean &lt;- sapply(h_seq, ar_1_mean_h, b0 = b0_hat, b1 = b1_hat, yT = y_obs[n_obs])\n    plug_sd   &lt;- sqrt(sapply(h_seq, ar_1_var_h, b1 = b1_hat, sigma = sigma_hat))\n    \n    # bootstrap\n    bootstrap_forecasts &lt;- matrix(NA, nrow = B, ncol = H + 1)\n    resid_centered &lt;- resid_hat - mean(resid_hat)\n    \n    for(b in 1:B){\n      e_star &lt;- sample(resid_centered, size = n_obs - 1, replace = TRUE)\n      y_star &lt;- numeric(n_obs)\n      y_star[1] &lt;- y_obs[1]\n      for(t in 2:n_obs){\n        y_star[t] &lt;- b0_hat + b1_hat * y_star[t-1] + e_star[t-1]\n      }\n      Ys &lt;- y_star[2:n_obs]\n      Xs &lt;- cbind(1, y_star[1:(n_obs-1)])\n      fit_star &lt;- lm(Ys ~ Xs - 1)\n      coef_star &lt;- coef(fit_star)\n      b0_star &lt;- coef_star[1]\n      b1_star &lt;- coef_star[2]\n      resid_star &lt;- resid(fit_star)\n      resid_star_centered &lt;- resid_star - mean(resid_star)\n      \n      y_fut &lt;- numeric(H + 1)\n      y_fut[1] &lt;- y_obs[n_obs]\n      future_shocks &lt;- sample(resid_star_centered, size = H, replace = TRUE)\n      for(h in 1:H){\n        y_fut[h + 1] &lt;- b0_star + b1_star * y_fut[h] + future_shocks[h]\n      }\n      bootstrap_forecasts[b, ] &lt;- y_fut\n    }\n    \n    boot_mean &lt;- colMeans(bootstrap_forecasts)\n    \n    sim_data(list(\n      y_obs = y_obs,\n      plug_mean = plug_mean,\n      plug_sd = plug_sd,\n      bootstrap_forecasts = bootstrap_forecasts,\n      boot_mean = boot_mean,\n      H = H\n    ))\n  }, ignoreNULL = FALSE)\n  \n  output$fanPlot &lt;- renderPlot({\n    dat &lt;- sim_data()\n    if(is.null(dat)) return(NULL)\n    \n    y_obs &lt;- dat$y_obs\n    plug_mean &lt;- dat$plug_mean\n    plug_sd &lt;- dat$plug_sd\n    bootstrap_forecasts &lt;- dat$bootstrap_forecasts\n    boot_mean &lt;- dat$boot_mean\n    H &lt;- dat$H\n    \n    n_obs &lt;- length(y_obs)\n    \n    # Window: last 20 obs and 20 forecasts\n    obs_window &lt;- (n_obs-19):n_obs\n    f_range &lt;- (n_obs):(n_obs + H)\n    plot_range &lt;- c((n_obs-19):(n_obs+H))\n    \n    y_min &lt;- min(c(y_obs[obs_window], plug_mean - 4 * plug_sd, bootstrap_forecasts))\n    y_max &lt;- max(c(y_obs[obs_window], plug_mean + 4 * plug_sd, bootstrap_forecasts))\n    \n    plot(plot_range, rep(NA, length(plot_range)), type = \"n\",\n         xlab = \"t\", ylab = expression(y[t]),\n         ylim = c(y_min, y_max), bty = \"n\",\n         main = \"AR(1) forecast: plug-in (red) vs residual-bootstrap (blue)\")\n    \n    rect(n_obs + 0.5, y_min, n_obs + H + 0.5, y_max,\n         col = rgb(0.85,0.85,0.85,0.5), border = NA)\n    \n    lines(obs_window, y_obs[obs_window], col = \"black\", lwd = 2)\n    \n    # bootstrap fan\n    if(input$show_blue){\n      prob_levels &lt;- c(0.001, 0.005, 0.01, seq(0.02, 0.48, by = 0.02))\n      lower_probs &lt;- prob_levels\n      upper_probs &lt;- 1 - prob_levels\n      boot_fan_lower &lt;- apply(bootstrap_forecasts, 2, quantile, probs = lower_probs)\n      boot_fan_upper &lt;- apply(bootstrap_forecasts, 2, quantile, probs = upper_probs)\n      for(i in seq_len(nrow(boot_fan_lower))){\n        polygon(c(f_range, rev(f_range)),\n                c(boot_fan_upper[i, ], rev(boot_fan_lower[i, ])),\n                col = rgb(0,0,1,0.08), border = NA)\n      }\n      lines(f_range, apply(bootstrap_forecasts, 2, median),\n            col = rgb(0,0,1,0.8), lty = 2, lwd = 1.5)\n      lines(f_range, boot_mean, col = rgb(0,0,1,0.9), lty = 1, lwd = 1)\n    }\n    \n    # plug-in fan\n    if(input$show_red){\n      alpha_vec &lt;- c(0.01, seq(0.1,0.9,by=0.1))\n      for(a in rev(alpha_vec)){\n        U &lt;- qnorm(1 - a/2, mean = plug_mean, sd = plug_sd)\n        L &lt;- qnorm(a/2, mean = plug_mean, sd = plug_sd)\n        polygon(c(f_range, rev(f_range)),\n                c(U, rev(L)),\n                col = rgb(1, 0, 0, 0.15), border = NA)\n      }\n      lines(f_range, plug_mean, col = \"red\", lty = 2, lwd = 2)\n    }\n    \n    abline(v = n_obs + 0.5, lty = 3)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "slides/ar-forecast.html#posterior-predictive-distribution",
    "href": "slides/ar-forecast.html#posterior-predictive-distribution",
    "title": "Probabilistic predictions",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\nAfter computing the posterior for parameters\n\\[\np(\\Btheta\\given y_{0:t})\n=\n\\frac{p(y_{1:t}\\given y_0\\com \\Btheta)p(\\Btheta)}{p(y_{1:t}\\given y_0)},\n\\]\n\nyou base forecasts on the posterior predictive distribution:\n\\[\np(y_{t+1}\\,|\\,y_{0:t})\n=\n\\int\np(y_{t+1}\\,|\\,y_{0:t},\\,\\boldsymbol{\\theta})\np(\\boldsymbol{\\theta}\\,|\\,y_{0:t})\n\\,\\text{d}\\boldsymbol{\\theta}.\n\\]\n\n\nImmediately incorporates both data and parameter uncertainty by construction."
  },
  {
    "objectID": "slides/ar-forecast.html#probabilistic-prediction-a-natural-byproduct-of-bayes",
    "href": "slides/ar-forecast.html#probabilistic-prediction-a-natural-byproduct-of-bayes",
    "title": "Probabilistic predictions",
    "section": "Probabilistic prediction: a natural byproduct of Bayes",
    "text": "Probabilistic prediction: a natural byproduct of Bayes\nIt falls out basically for free:\n\n\n\n\n\n\n\nBjørnstad, Jan (1990): “Predictive likelihood: a review,” Statistical Science\n\n\n“Prediction of the value of an unobserved or future random variable is a fundamental problem in statistics. From a Bayesian point of view, it is solved in a straightforward manner by finding the posterior predictive density of the unobserved random variable given the data. If one does not want to pay the Bayesian price of having to determine a prior, no unifying basis for prediction has existed until recently.”\n\n\n\n\n\n“If one does not want to pay the Bayesian price of having to determine a prior…” 🙄\n\n\n\n\n\n\n\n\nAndrew Gelman, coming in hot\n\n\n“The anti-Bayesian is standing at the back window with a shotgun, scanning for priors coming over the hill, while a million assumptions just walk right into his house through the front door.”"
  },
  {
    "objectID": "slides/ar-forecast.html#conjugate-bayes-behaves-like-iid-regression",
    "href": "slides/ar-forecast.html#conjugate-bayes-behaves-like-iid-regression",
    "title": "Probabilistic predictions",
    "section": "Conjugate Bayes behaves like iid regression",
    "text": "Conjugate Bayes behaves like iid regression\nA conjugate normal-inverse-gamma prior begets a conjugate posterior:\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t}\n&\\sim\n\\text{IG}(a_t\\com b_t)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:t}\n&\\sim\n\\text{N}_2(\\Bm_t\\com\\sigma^2\\BH^{-1}_t)\n\\\\\ny_{t+1}\\given\\Bbeta\\com \\sigma^2\\com y_{0:t}\n&\\sim \\N(\\Bx_{t+1}^\\tr\\Bbeta\\com\\sigma^2),\\quad \\Bx_{t+1}=\\begin{bmatrix}1 & y_{t}\\end{bmatrix}^\\tr.\n\\end{aligned}\n\\]\n\nThe one-step posterior predictive distribution is non-standard Student’s \\(t\\):\n\\[\n\\begin{aligned}\ny_{t+1}\\given y_{0:t}\n&\\sim\nt(\\nu_{t+1|t}\\com\\bar{y}_{t+1|t}\\com s_{t+1|t}^2)\n\\\\\n\\\\\n\\nu_{t+1|t}\n&=\n2a_t\n\\\\\n\\bar{y}_{t+1|t}\n&=\n\\Bx_{t+1}^\\tr\\Bm_t\n\\\\\ns_{t+1|t}^2\n&=\n\\frac{b_t}{a_t}\n(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\n.\n\\end{aligned}\n\\]\nTo forecast farther out in time, you need to simulate."
  },
  {
    "objectID": "slides/ar-forecast.html#one-step-ahead-probabilistic-prediction",
    "href": "slides/ar-forecast.html#one-step-ahead-probabilistic-prediction",
    "title": "Probabilistic predictions",
    "section": "One-step-ahead probabilistic prediction",
    "text": "One-step-ahead probabilistic prediction\n\nOur density forecast is:\n\\[\ny_{t+1}\\mid y_{0:t}\n\\sim\nt(\\nu_{t+1|t}\\com\\bar{y}_{t+1|t}\\com s_{t+1|t}^2).\n\\]\nThe moments are\n\\[\n\\begin{aligned}\nE(y_{t+1}\\mid y_{0:t})\n&=\n\\bar{y}_{t+1|t}\n,\n&&\n\\nu_{t+1|t}&gt;1\n\\\\\n\\text{var}(y_{t+1}\\mid y_{0:t})\n&=\n\\frac{\\nu_{t+1|t}}{\\nu_{t+1|t}-2}\ns_{t+1|t}^2\n,\n&&\n\\nu_{t+1|t}&gt;2.\n\\end{aligned}\n\\]\nUse quantiles to get prediction intervals."
  },
  {
    "objectID": "slides/ar-forecast.html#how-do-you-get-full-predictive-distributions",
    "href": "slides/ar-forecast.html#how-do-you-get-full-predictive-distributions",
    "title": "Probabilistic predictions",
    "section": "How do you get full predictive distributions?",
    "text": "How do you get full predictive distributions?\n\nIn general, use simulation:\n\nClassical approach: bootstrapping;\nBayesian approach: posterior predictive simulation.\n\n\n\nEither way, you get Monte Carlo draws from a forecast distribution:\n\\[\n\\tilde{y}_{t+h}^{(1)}\\com \\tilde{y}_{t+h}^{(2)}\\com ...\\com \\tilde{y}_{t+h}^{(k)}\n\\sim \\hat{F}_{t+h|t}.\n\\]\n\n\nWhat do you do with them?"
  },
  {
    "objectID": "slides/ar-forecast.html#probabilistic-forecasting-via-monte-carlo",
    "href": "slides/ar-forecast.html#probabilistic-forecasting-via-monte-carlo",
    "title": "Probabilistic predictions",
    "section": "Probabilistic forecasting via Monte Carlo",
    "text": "Probabilistic forecasting via Monte Carlo\nUse the simulations \\(\\tilde{y}_{t+h}^{(1:k)}=\\left\\{\\tilde{y}_{t+h}^{(1)}\\com \\tilde{y}_{t+h}^{(2)}\\com ...\\com \\tilde{y}_{t+h}^{(k)}\\right\\}\\) to construct whatever forecast object you want:\n\n\\[\n\\hat{y}_{t+h|t}=\\frac{1}{k}\\sum\\limits_{j=1}^k\\tilde{y}_{t+h}^{(j)}. \\quad (\\text{...or median})\n\\]\n\n\nForecast interval:\n\\[\n\\hat{I}_{t+h|t} = \\left[\\hat{Q}_{\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\com \\hat{Q}_{1-\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\right]. \\quad (...\\text{or hdi})\n\\]\n\n\nForecast distribution:\n\\[\n\\hat{f}_{t+h|t}\n=\n\\text{histogram}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right). \\quad (...\\text{or kde})\n\\]"
  },
  {
    "objectID": "slides/ar-forecast.html#how-do-you-evaluate-the-forecasts",
    "href": "slides/ar-forecast.html#how-do-you-evaluate-the-forecasts",
    "title": "Probabilistic predictions",
    "section": "How do you evaluate the forecasts?",
    "text": "How do you evaluate the forecasts?\nYou generate a sequence of one-step-ahead predictions:\n\\[\n\\begin{matrix}\n\\hat{y}_{1|0} & \\hat{y}_{2|1} & \\hat{y}_{3|2} & \\hat{y}_{4|3} & \\hat{y}_{5|4} & \\cdots&\\hat{y}_{t|t-1} & \\cdots\\\\\n\\hat{I}_{1|0} & \\hat{I}_{2|1} & \\hat{I}_{3|2} & \\hat{I}_{4|3} & \\hat{I}_{5|4} & \\cdots&\\hat{I}_{t|t-1} & \\cdots\\\\\n\\hat{f}_{1|0} & \\hat{f}_{2|1} & \\hat{f}_{3|2} & \\hat{f}_{4|3} & \\hat{f}_{5|4} & \\cdots&\\hat{f}_{t|t-1} & \\cdots\n\\end{matrix}\n\\]\n\nBut then the data you were trying to forecast eventually arrive:\n\\[\n\\begin{matrix}\ny_1 & y_2 & y_3 & y_4 & y_5 & \\cdots &y_t & \\cdots\n\\end{matrix}\n\\]\nHow do we score the forecasts and summarize?"
  },
  {
    "objectID": "slides/ar-forecast.html#the-game",
    "href": "slides/ar-forecast.html#the-game",
    "title": "Probabilistic predictions",
    "section": "The game",
    "text": "The game\nWe will illustrate by comparing the performance of two well-behaved methods:\nA. classical predictive distribution from iid normal model;\nB. posterior predictive distribution from conjugate, Gaussian AR(1).\nThere will be two running examples:\n1. Simulated data from AR(1)\n\n    - Method A (iid normal) is wrong by construction;\n    - Method B is right by construction;\n    \n2. Quarterly real GDP growth\n\n    - both methods are \"wrong,\" but is one strictly preferred?\nOur forecast metrics will tease all of that out."
  },
  {
    "objectID": "slides/ar-forecast.html#method-a",
    "href": "slides/ar-forecast.html#method-a",
    "title": "Probabilistic predictions",
    "section": "Method A",
    "text": "Method A\n\nAssume\n\n\n\\[\ny_1\\com y_2\\com ...\\com y_n\\com y_{n+1}\\iid\\N(\\mu\\com\\sigma^2).\n\\]\n\n\nWe know \\(\\bar{y}_n\\sim\\N(\\mu\\com \\sigma^2/n)\\) independent of \\(y_{n+1}\\), and so:\n\\[\n\\frac{\\bar{y}_n-y_{n+1}}{\\sigma\\sqrt{1+\\frac{1}{n}}}\\sim\\N(0\\com 1)\n\\quad\n\\implies\n\\quad\n\\frac{\\bar{y}_n-y_{n+1}}{\\hat{\\sigma}\\sqrt{1+\\frac{1}{n}}}\\sim t_{n-1}.\n\\]\n\n\nThe (classical) predictive distribution for the next \\(y_{n+1}\\) is:\n\\[\nt\\left(n-1\\com \\bar{y}_n\\com \\widehat{\\sigma^2}\\left(1+\\frac{1}{n}\\right)\\right).\n\\]\nThis incorporates sampling uncertainty for the mean and variance, and the inherent uncertainty in new \\(y_{n+1}\\)."
  },
  {
    "objectID": "slides/ar-forecast.html#method-b-you-know-the-drill",
    "href": "slides/ar-forecast.html#method-b-you-know-the-drill",
    "title": "Probabilistic predictions",
    "section": "Method B: you know the drill",
    "text": "Method B: you know the drill\nA conjugate normal-inverse-gamma prior begets a conjugate posterior:\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t}\n&\\sim\n\\text{IG}(a_t\\com b_t)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:t}\n&\\sim\n\\text{N}_2(\\Bm_t\\com\\sigma^2\\BH^{-1}_t)\n\\\\\ny_{t+1}\\given\\Bbeta\\com \\sigma^2\\com y_{0:t}\n&\\sim \\N(\\Bx_{t+1}^\\tr\\Bbeta\\com\\sigma^2),\\quad \\Bx_{t+1}=\\begin{bmatrix}1 & y_{t}\\end{bmatrix}^\\tr.\n\\end{aligned}\n\\]\n\nThe one-step posterior predictive distribution is non-standard Student’s \\(t\\):\n\\[\n\\begin{aligned}\ny_{t+1}\\given y_{0:t}\n&\\sim\nt(\\nu_{t+1|t}\\com\\bar{y}_{t+1|t}\\com s_{t+1|t}^2)\n\\\\\n\\\\\n\\nu_{t+1|t}\n&=\n2a_t\n\\\\\n\\bar{y}_{t+1|t}\n&=\n\\Bx_{t+1}^\\tr\\Bm_t\n\\\\\ns_{t+1|t}^2\n&=\n\\frac{b_t}{a_t}\n(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ar-forecast.html#dataset-1-simulated",
    "href": "slides/ar-forecast.html#dataset-1-simulated",
    "title": "Probabilistic predictions",
    "section": "Dataset 1: simulated",
    "text": "Dataset 1: simulated"
  },
  {
    "objectID": "slides/ar-forecast.html#forecast-distributions-from-iid-normal",
    "href": "slides/ar-forecast.html#forecast-distributions-from-iid-normal",
    "title": "Probabilistic predictions",
    "section": "Forecast distributions from iid normal",
    "text": "Forecast distributions from iid normal"
  },
  {
    "objectID": "slides/ar-forecast.html#forecast-distributions-from-bayesian-ar1",
    "href": "slides/ar-forecast.html#forecast-distributions-from-bayesian-ar1",
    "title": "Probabilistic predictions",
    "section": "Forecast distributions from Bayesian AR(1)",
    "text": "Forecast distributions from Bayesian AR(1)"
  },
  {
    "objectID": "slides/ar-forecast.html#dataset-2-us-quarterly-real-gdp-growth",
    "href": "slides/ar-forecast.html#dataset-2-us-quarterly-real-gdp-growth",
    "title": "Probabilistic predictions",
    "section": "Dataset 2: US quarterly real GDP growth",
    "text": "Dataset 2: US quarterly real GDP growth"
  },
  {
    "objectID": "slides/ar-forecast.html#forecast-distributions-from-iid-normal-1",
    "href": "slides/ar-forecast.html#forecast-distributions-from-iid-normal-1",
    "title": "Probabilistic predictions",
    "section": "Forecast distributions from iid normal",
    "text": "Forecast distributions from iid normal"
  },
  {
    "objectID": "slides/ar-forecast.html#forecast-distributions-from-bayesian-ar1-1",
    "href": "slides/ar-forecast.html#forecast-distributions-from-bayesian-ar1-1",
    "title": "Probabilistic predictions",
    "section": "Forecast distributions from Bayesian AR(1)",
    "text": "Forecast distributions from Bayesian AR(1)"
  },
  {
    "objectID": "slides/ar-forecast.html#any-ideas",
    "href": "slides/ar-forecast.html#any-ideas",
    "title": "Probabilistic predictions",
    "section": "Any ideas?",
    "text": "Any ideas?"
  },
  {
    "objectID": "slides/ar-forecast.html#point-prediction",
    "href": "slides/ar-forecast.html#point-prediction",
    "title": "Probabilistic predictions",
    "section": "Point prediction",
    "text": "Point prediction\nWe want the point prediction that minimizes expected loss:\n\n\\[\n\\hat{y}_{t+1|t}\n\\;=\\;\n\\argmin{\\hat{y}\\in\\mathbb{R}}\n\\; E\\big[\\, L\\big(y_{t+1},\\,\\hat{y}\\big) \\,\\big|\\, y_{0:t} \\big].\n\\]\n\n\nThe expectation is taken with respect to the “true” or “idealized” conditional distribution \\(p(y_{t+1}\\given y_{0:t})\\), which we don’t know.\n\n\nWe approximate it with whatever forecast distribution we’ve generated."
  },
  {
    "objectID": "slides/ar-forecast.html#picking-a-loss-function",
    "href": "slides/ar-forecast.html#picking-a-loss-function",
    "title": "Probabilistic predictions",
    "section": "Picking a loss function",
    "text": "Picking a loss function\n\nWe have nice results for some loss functions:\n\n\n\\[\n\\begin{array}{rcl}\nL(y_{t+1},\\hat{y}) = (y_{t+1} - \\hat{y})^2\n& \\implies &\n\\hat{y}_{t+1|t} = E[\\,y_{t+1}\\mid y_{0:t}\\,] \\\\[1.2em]\nL(y_{t+1},\\hat{y}) = |y_{t+1} - \\hat{y}|\n& \\implies &\n\\hat{y}_{t+1|t} = \\operatorname{median}(y_{t+1}\\mid y_{0:t}).\n\\end{array}\n\\]\n\n\nAnd there are many more where that came from."
  },
  {
    "objectID": "slides/ar-forecast.html#in-practice",
    "href": "slides/ar-forecast.html#in-practice",
    "title": "Probabilistic predictions",
    "section": "In practice",
    "text": "In practice\nMetrics for scoring the average quality of the point predictions over time:\n\\[\n\\begin{aligned}\n\\text{MSFE}\n&=\n\\frac{1}{T}\n\\sum\\limits_{t=1}^T\n(y_t-\\hat{y}_{t|t-1})^2\n\\\\\n\\text{MAFE}\n&=\n\\frac{1}{T}\n\\sum\\limits_{t=1}^T\n|y_t-\\hat{y}_{t|t-1}|.\n\\end{aligned}\n\\]\nWe want these to be small.\n\n\n\n\n\n\n\nMake sure your loss function and your point prediction play nice\n\n\n\nIf you’re looking at MAFE, use forecast median;\nIf you’re looking at MSFE, use the forecast mean."
  },
  {
    "objectID": "slides/ar-forecast.html#our-simulated-data",
    "href": "slides/ar-forecast.html#our-simulated-data",
    "title": "Probabilistic predictions",
    "section": "Our simulated data",
    "text": "Our simulated data\nMSE of forecast mean:\n\nmean((y - pred_params_iid_sim[,\"location\"])^2, na.rm = TRUE)\n\n[1] 162.4966\n\nmean((y - pred_params_ar1_sim[,\"location\"])^2, na.rm = TRUE)\n\n[1] 4.162699\n\n\n\nMAE of forecast median (same as mean for these methods):\n\nmean(abs(y - pred_params_iid_sim[,\"location\"]), na.rm = TRUE)\n\n[1] 10.03766\n\nmean(abs(y - pred_params_ar1_sim[,\"location\"]), na.rm = TRUE)\n\n[1] 1.637622"
  },
  {
    "objectID": "slides/ar-forecast.html#our-real-data",
    "href": "slides/ar-forecast.html#our-real-data",
    "title": "Probabilistic predictions",
    "section": "Our real data",
    "text": "Our real data\nMSE of forecast mean:\n\nmean((rgdp - pred_params_iid_real[,\"location\"])^2, na.rm = TRUE)\n\n[1] 6.66254\n\nmean((rgdp - pred_params_ar1_real[,\"location\"])^2, na.rm = TRUE)\n\n[1] 2.582788\n\n\n\nMAE of forecast median (same as mean for these methods):\n\nmean(abs(rgdp - pred_params_iid_real[,\"location\"]), na.rm = TRUE)\n\n[1] 1.861102\n\nmean(abs(rgdp - pred_params_ar1_real[,\"location\"]), na.rm = TRUE)\n\n[1] 1.054689"
  },
  {
    "objectID": "slides/ar-forecast.html#any-ideas-1",
    "href": "slides/ar-forecast.html#any-ideas-1",
    "title": "Probabilistic predictions",
    "section": "Any ideas?",
    "text": "Any ideas?"
  },
  {
    "objectID": "slides/ar-forecast.html#interval-width-and-coverage",
    "href": "slides/ar-forecast.html#interval-width-and-coverage",
    "title": "Probabilistic predictions",
    "section": "Interval width and coverage",
    "text": "Interval width and coverage\n\nYou want intervals that are small enough to be informative, but large enough to swallow the truth often, and there’s a trade-off.\n\n\\(\\hat{I}=(-\\infty\\com \\infty)\\) always has perfect coverage but teaches you nothing;\nLook at average size and empirical coverage:\n\n\n\\[\n\\begin{aligned}\n\\overline{\\text{Size}}\n&= \\frac{1}{T} \\sum_{t=1}^{T} \\Big( \\hat{I}_{t\\mid t-1}^{\\text{upper}} - \\hat{I}_{t\\mid t-1}^{\\text{lower}} \\Big), \\\\[0.8em]\n\\overline{\\text{Coverage}}\n&= \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{1}\\Big( y_t \\in \\hat{I}_{t\\mid t-1} \\Big).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ar-forecast.html#interval-performance-on-simulated-data",
    "href": "slides/ar-forecast.html#interval-performance-on-simulated-data",
    "title": "Probabilistic predictions",
    "section": "Interval performance on simulated data",
    "text": "Interval performance on simulated data\n\nSize of 90% intervals:\n\nmean(PI_iid_sim[,1], na.rm = TRUE)   \n\n[1] 37.61362\n\nmean(PI_ar1_sim[,1], na.rm = TRUE)   \n\n[1] 6.660308\n\n\n\n\nCoverage of 90% intervals:\n\nmean(PI_iid_sim[,2], na.rm = TRUE)   \n\n[1] 0.8537415\n\nmean(PI_ar1_sim[,2], na.rm = TRUE)   \n\n[1] 0.8991798"
  },
  {
    "objectID": "slides/ar-forecast.html#interval-performance-on-gdp-growth",
    "href": "slides/ar-forecast.html#interval-performance-on-gdp-growth",
    "title": "Probabilistic predictions",
    "section": "Interval performance on GDP growth",
    "text": "Interval performance on GDP growth\n\nSize of 90% intervals:\n\nmean(PI_iid_real[,1], na.rm = TRUE)   \n\n[1] 9.922717\n\nmean(PI_ar1_real[,1], na.rm = TRUE)   \n\n[1] 5.451231\n\n\n\n\nCoverage of 90% intervals:\n\nmean(PI_iid_real[,2], na.rm = TRUE)   \n\n[1] 0.9320388\n\nmean(PI_ar1_real[,2], na.rm = TRUE)   \n\n[1] 0.9387097\n\n\n\n\nIn this case it turns out that you get the same (over-)coverage from either procedure, but the AR(1) intervals are smaller on average."
  },
  {
    "objectID": "slides/ar-forecast.html#interval-score",
    "href": "slides/ar-forecast.html#interval-score",
    "title": "Probabilistic predictions",
    "section": "Interval score",
    "text": "Interval score\nAverage over time for an holistic metric of interval performance:\n\\[\n\\mathrm{IS}_\\alpha(l,u; y)\n=\n(u - l)\n+\n\\frac{2}{\\alpha}\\,(l - y)\\,\\mathbf{1}(y &lt; l)\n+\n\\frac{2}{\\alpha}\\,(y - u)\\,\\mathbf{1}(y &gt; u).\n\\]\nSynthesizes both size and coverage, but in practice, if you want to understand why the score was good or bad, you have to crack it open and look at the size and coverage components separately anyway."
  },
  {
    "objectID": "slides/ar-forecast.html#any-ideas-2",
    "href": "slides/ar-forecast.html#any-ideas-2",
    "title": "Probabilistic predictions",
    "section": "Any ideas?",
    "text": "Any ideas?"
  },
  {
    "objectID": "slides/ar-forecast.html#recap-probability-integral-transform-pit",
    "href": "slides/ar-forecast.html#recap-probability-integral-transform-pit",
    "title": "Probabilistic predictions",
    "section": "Recap: probability integral transform (PIT)",
    "text": "Recap: probability integral transform (PIT)\nTake a random variable \\(Y\\sim F\\) and plug it into its own cdf to define a new random variable \\(U=F(Y)\\):"
  },
  {
    "objectID": "slides/ar-forecast.html#recap-probability-integral-transform-pit-1",
    "href": "slides/ar-forecast.html#recap-probability-integral-transform-pit-1",
    "title": "Probabilistic predictions",
    "section": "Recap: probability integral transform (PIT)",
    "text": "Recap: probability integral transform (PIT)\nIf you take a (continuous) random variable \\(Y\\sim F\\) and plug it into its own cdf to define a new random variable \\(U=F(Y)\\), then the new thing has:\n\n\\[\nU\\sim \\text{Unif}(0\\com 1).\n\\]\n\n\nFix any \\(u\\in(0\\com 1)\\). Then the cdf of the new random variable \\(U\\) is:\n\n\n\\[\nP(U\\leq u)=P(F(Y)\\leq u)=P(Y\\leq F^{-1}(u))=F(F^{-1}(u))=u.\n\\]\n\n\nThat’s the cdf of Unif(0, 1)."
  },
  {
    "objectID": "slides/ar-forecast.html#dont-believe-me",
    "href": "slides/ar-forecast.html#dont-believe-me",
    "title": "Probabilistic predictions",
    "section": "Don’t believe me?",
    "text": "Don’t believe me?\n\nx &lt;- rnorm(10000)\nu &lt;- pnorm(x)\nhist(u, breaks = \"Scott\", freq = FALSE)\nabline(h = 1, col = \"red\")"
  },
  {
    "objectID": "slides/ar-forecast.html#whats-pit-got-to-do-with-it",
    "href": "slides/ar-forecast.html#whats-pit-got-to-do-with-it",
    "title": "Probabilistic predictions",
    "section": "What’s PIT got to do with it?",
    "text": "What’s PIT got to do with it?\nLet \\(G_t\\) be the “true” cdf that nature is drawing from to produce \\(y_t\\). By the probability integral transform, we know that:\n\n\\[\nG_1(y_1)\\com G_2(y_2)\\com ...\\com G_t(y_t)\\com ...\\sim\\text{Unif}(0\\com 1).\n\\]\n\n\nIt would be amazing if our method produced exactly correct predictive distributions: \\(\\hat{F}_{t|t-1}=G_t\\). We’re probably not so lucky, but if we’re in the ballpark, then we should see:\n\n\n\\[\n\\hat{F}_{1|0}(y_1)\\com \\hat{F}_{2|1}(y_2)\\com ...\\com \\hat{F}_{t|t-1}(y_t)\\com ...\\sim\\text{Unif}(0\\com 1).\n\\]\n\n\nLet’s check!"
  },
  {
    "objectID": "slides/ar-forecast.html#iid-normal-method-on-simulated-data",
    "href": "slides/ar-forecast.html#iid-normal-method-on-simulated-data",
    "title": "Probabilistic predictions",
    "section": "iid normal method on simulated data",
    "text": "iid normal method on simulated data"
  },
  {
    "objectID": "slides/ar-forecast.html#bayesian-ar1-on-simulated-data",
    "href": "slides/ar-forecast.html#bayesian-ar1-on-simulated-data",
    "title": "Probabilistic predictions",
    "section": "Bayesian AR(1) on simulated data",
    "text": "Bayesian AR(1) on simulated data\n\n\n\n\n\n\n\n\n\nRemember: up to estimation error, this method is exactly correct by construction."
  },
  {
    "objectID": "slides/ar-forecast.html#iid-normal-method-on-gdp-growth",
    "href": "slides/ar-forecast.html#iid-normal-method-on-gdp-growth",
    "title": "Probabilistic predictions",
    "section": "iid normal method on GDP growth",
    "text": "iid normal method on GDP growth\n\n\n\n\n\n\n\n\nClearly not uniform."
  },
  {
    "objectID": "slides/ar-forecast.html#bayesian-ar1-on-gdp-growth",
    "href": "slides/ar-forecast.html#bayesian-ar1-on-gdp-growth",
    "title": "Probabilistic predictions",
    "section": "Bayesian AR(1) on GDP growth",
    "text": "Bayesian AR(1) on GDP growth\n\n\n\n\n\n\n\n\n\nWhat does it mean that these have a peak in the middle?"
  },
  {
    "objectID": "slides/ar-forecast.html#diagnosing-underover-dispersion",
    "href": "slides/ar-forecast.html#diagnosing-underover-dispersion",
    "title": "Probabilistic predictions",
    "section": "Diagnosing under/over-dispersion",
    "text": "Diagnosing under/over-dispersion"
  },
  {
    "objectID": "slides/ar-forecast.html#what-do-the-pit-values-tell-you",
    "href": "slides/ar-forecast.html#what-do-the-pit-values-tell-you",
    "title": "Probabilistic predictions",
    "section": "What do the PIT values tell you?",
    "text": "What do the PIT values tell you?\nWhere did the true \\(y\\) fall under your predictive distribution?"
  },
  {
    "objectID": "slides/ar-forecast.html#what-do-the-pit-values-tell-you-1",
    "href": "slides/ar-forecast.html#what-do-the-pit-values-tell-you-1",
    "title": "Probabilistic predictions",
    "section": "What do the PIT values tell you?",
    "text": "What do the PIT values tell you?\n\nIf \\(y\\) tends to surprise in the left tail, we’ll get too many PITs near 0;\nIf \\(y\\) tends to surprise in the right tail, we’ll get too many PITs near 1;\nIf \\(y\\) tends to surprise in the middle, we’ll get too many PITs near 0.5;\nIf the forecast distributions tend to be overdispersed (too much mass in the tails), the histogram is hump-shaped;\nIf the forecast distributions tend to be underdispersed (not enough mass in the tails), the histogram is u-shaped."
  },
  {
    "objectID": "slides/ar-forecast.html#summary-calibration",
    "href": "slides/ar-forecast.html#summary-calibration",
    "title": "Probabilistic predictions",
    "section": "Summary: calibration",
    "text": "Summary: calibration\nIf your sequence of forecast distributions is well-calibrated, then the PITs should be approximately uniformly distributed:\n\\[\n\\hat{F}_{1|0}(y_1)\\com \\hat{F}_{2|1}(y_2)\\com ...\\com \\hat{F}_{t|t-1}(y_t)\\com ...\\sim\\text{Unif}(0\\com 1).\n\\]\n\nCheck it with a histogram, QQ-plot, goodness-of-fit test…\n\n\nDeviations from uniformity provide useful diagnostic information.\n\n\n\n\n\n\n\n\nThis is necessary but not sufficient!\n\n\nCalibration alone is not enough to distinguish good/better/best forecasts."
  },
  {
    "objectID": "slides/ar-forecast.html#maximize-sharpness-subject-to-calibration",
    "href": "slides/ar-forecast.html#maximize-sharpness-subject-to-calibration",
    "title": "Probabilistic predictions",
    "section": "Maximize sharpness subject to calibration",
    "text": "Maximize sharpness subject to calibration\n\nYou want a forecasting method to be calibrated;\nIf you have many methods to choose from, all of which appear calibrated, select the one that is the sharpest;\n\nSharpness refers to how concentrated the forecast distributions are. Among calibrated distributions, you want the one that is sharpest, most decisive, most concentrated;\nSharpness can be measured by your preferred measure of spread: variance, IQR, etc."
  },
  {
    "objectID": "slides/ar-forecast.html#sharpness-for-the-simulated-data",
    "href": "slides/ar-forecast.html#sharpness-for-the-simulated-data",
    "title": "Probabilistic predictions",
    "section": "Sharpness for the simulated data",
    "text": "Sharpness for the simulated data\nCompare the scale parameters of the predictive distributions:"
  },
  {
    "objectID": "slides/ar-forecast.html#sharpness-for-the-gdp-growth",
    "href": "slides/ar-forecast.html#sharpness-for-the-gdp-growth",
    "title": "Probabilistic predictions",
    "section": "Sharpness for the GDP growth",
    "text": "Sharpness for the GDP growth"
  },
  {
    "objectID": "slides/ar-forecast.html#log-predictive-score",
    "href": "slides/ar-forecast.html#log-predictive-score",
    "title": "Probabilistic predictions",
    "section": "Log predictive score",
    "text": "Log predictive score\nEvaluates if the forecast distribution placed high mass/density on the region where \\(y_t\\) actually showed up:\n\\[\n\\overline{\\text{LPS}} = \\frac{1}{T} \\sum_{t=1}^{T} \\ln \\hat{f}_{t|t-1}(y_t).\n\\]\n\nBigger is better;\nSimultaneously rewards both calibration and sharpness;\n\nProper scoring rule: encourages honest probabilistic predictions;\nLocal measure of quality. There are also global measures like the continuous ranked probability score (CRPS)."
  },
  {
    "objectID": "slides/ar-forecast.html#lps-rewards-both-calibration-and-sharpness",
    "href": "slides/ar-forecast.html#lps-rewards-both-calibration-and-sharpness",
    "title": "Probabilistic predictions",
    "section": "LPS rewards both calibration and sharpness",
    "text": "LPS rewards both calibration and sharpness"
  },
  {
    "objectID": "slides/ar-forecast.html#for-our-examples",
    "href": "slides/ar-forecast.html#for-our-examples",
    "title": "Probabilistic predictions",
    "section": "For our examples",
    "text": "For our examples\n\nSimulated data:\n\naverage_log_score(y, pred_params_iid_sim)\n\n[1] -3.965963\n\naverage_log_score(y, pred_params_ar1_sim)\n\n[1] -2.132817\n\n\n\n\nReal data:\n\naverage_log_score(rgdp, pred_params_iid_real)\n\n[1] -2.373455\n\naverage_log_score(rgdp, pred_params_ar1_real)\n\n[1] -1.886891"
  },
  {
    "objectID": "slides/ar-forecast.html#authors-and-papers-to-know",
    "href": "slides/ar-forecast.html#authors-and-papers-to-know",
    "title": "Probabilistic predictions",
    "section": "Authors and papers to know",
    "text": "Authors and papers to know\n\nGneiting & Raftery (2007): “Strictly Proper Scoring Rules, Prediction, and Estimation,” JASA;\n\nHard to read, but packed with useful info;\n\n\nGneiting, Balabdaoui, & Raftery (2007): “Probabilistic Forecasts, Calibration and Sharpness,” JRSSB;\n\n“We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration.” Bada bing."
  },
  {
    "objectID": "slides/ar-forecast.html#software-to-check-out",
    "href": "slides/ar-forecast.html#software-to-check-out",
    "title": "Probabilistic predictions",
    "section": "Software to check out",
    "text": "Software to check out\n\nlibrary(scoringRules)\nlibrary(scoringutils)"
  },
  {
    "objectID": "slides/ar-forecast.html#bootstrapping-for-dependent-data-with-iid-errors",
    "href": "slides/ar-forecast.html#bootstrapping-for-dependent-data-with-iid-errors",
    "title": "Probabilistic predictions",
    "section": "Bootstrapping for dependent data with iid errors",
    "text": "Bootstrapping for dependent data with iid errors\n\n\nData come from an AR(1) with mean zero iid errors (may not be normal!):\n\n\n\n\\[\ny_t\n=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\overset{\\text{iid}}{\\sim}F;\n\\]\n\n\n\nUse observed \\(y_{0:T}\\) to compute OLS estimate \\(\\hat{\\Bbeta}_T\\);\n\n\n\n\nEstimate residuals and center them:\n\n\\[\n\\hat{\\varepsilon}_t=y_t-\\hat{\\beta}_0-\\hat{\\beta}_1y_{t-1}\\quad \\to\\quad e_t=\\hat{\\varepsilon}_t-\\sum\\limits_{j=1}^T\\hat{\\varepsilon}_j/T.\n\\]\n\n\n\nConstruct alternative time series by resampling residuals:\n\n\\[\n\\begin{aligned}\n\\tilde{y}_0&=y_0\\\\\n\\tilde{y}_t&=\\hat{\\beta}_0+\\hat{\\beta}_1\\tilde{y}_{t-1}+\\tilde{e}_{t},&&\\tilde{e}_{t}\\overset{\\text{iid}}{\\sim} \\hat{F}_T.\n\\end{aligned}\n\\]\n\n\nRepeats the last step many times, and from then on it’s bootstrap like normal."
  },
  {
    "objectID": "slides/ar-forecast.html#bootstrapping-the-residuals",
    "href": "slides/ar-forecast.html#bootstrapping-the-residuals",
    "title": "Probabilistic predictions",
    "section": "Bootstrapping the residuals",
    "text": "Bootstrapping the residuals\nGird your loins:\n\n\\[\n\\begin{matrix}\n\\text{1. Original data} &&& y_{0:T} && \\\\\n&&& \\downarrow && \\\\\n\\text{2. OLS} &&& \\hat{\\Btheta}_{T} && \\\\\n&&& \\downarrow && \\\\\n\\text{3. Estimate (centered) residuals} &&& e_{1:T} && \\\\\n&\\swarrow &\\swarrow& \\cdots &\\searrow&\\searrow \\\\\n\\text{4. Resample residuals}&\\tilde{e}_{1:T}^{(1)} &\\tilde{e}_{1:T}^{(2)}& \\cdots &\\tilde{e}_{1:T}^{(k-1)}&\\tilde{e}_{1:T}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{5. Bootstrap time series}&\\tilde{y}_{0:T}^{(1)} &\\tilde{y}_{0:T}^{(2)}& \\cdots &\\tilde{y}_{0:T}^{(k-1)}&\\tilde{y}_{0:T}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{6. Bootstrap estimates}&\\tilde{\\Btheta}_{T}^{(1)} &\\tilde{\\Btheta}_{T}^{(2)}& \\cdots &\\tilde{\\Btheta}_{T}^{(k-1)}&\\tilde{\\Btheta}_{T}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{7. Draw more residuals}&\\tilde{e}_{T+1:T+h}^{(1)} &\\tilde{e}_{T+1:T+h}^{(2)}& \\cdots &\\tilde{e}_{T+1:T+h}^{(k-1)}&\\tilde{e}_{T+1:T+h}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{8. Simulate forecasts}&\\tilde{y}_{T+1:T+h}^{(1)} &\\tilde{y}_{T+1:T+h}^{(2)}& \\cdots &\\tilde{y}_{T+1:T+h}^{(k-1)}&\\tilde{y}_{T+1:T+h}^{(k)} \\\\\n\\end{matrix}\n\\]\n\n\nIn other words, it’s awful."
  },
  {
    "objectID": "slides/ar-forecast.html#and-thats-just-the-tip-of-the-iceberg",
    "href": "slides/ar-forecast.html#and-thats-just-the-tip-of-the-iceberg",
    "title": "Probabilistic predictions",
    "section": "And that’s just the tip of the iceberg",
    "text": "And that’s just the tip of the iceberg\n\n\n\n\nIf you really want to know,\nLahiri is your man."
  },
  {
    "objectID": "syllabus/syllabus-assignments.html",
    "href": "syllabus/syllabus-assignments.html",
    "title": "Assignments and grading",
    "section": "",
    "text": "Course grade\nYour final course grade breaks down as follows:\n\n[10%] Attendance and participation: we will be taking attendance at all class meetings;\n[15%] Final project: the semester ends with a data analysis project completed individually. Details to come;\n[75%] Case studies: this course is organized around a sequence of case studies where you work in teams to analyze complex, messy datasets coming from various domains. There will be 4 - 6 case studies depending on the pace of course, and they will each get equal weight in determining this course component. Within each case study, your grade breaks down as follows:\n\n(60%) written report;\n(30%) presentation;\n(10%) student-specific collaboration score.\n\n\nAt the end of the semester, your final letter grade will be determined using the usual thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese thresholds will not be adjusted upward, but they may be adjusted downward in your favor.\n\n\n\n\nCase studies\nThe case studies are the heart of the class. Case studies will last 2 - 4 weeks depending on complexity, and there will be 4 - 6 in total. Sometimes the class as a whole requests more time to work on the more involved cases, and so we are prepared to be flexible and adjust the pace of the course as needed. Hence, the final number of cases is TBD.\nEach case study will go something like this:\n\nthe instructor introduces the data and the research question in lecture;\nstudents are randomly sorted into teams of 2 - 4. The teams are different for every case study;\njust like in STA 199, we share a GitHub repo with each team that contains the necessary data as well as template Quarto files for your presentation and report;\nworking inside their GitHub repo, teams use R and Quarto to collaborate on a written report and presentation slides;\nduring lecture, the instructor introduces new statistical topic that teams may find relevant to the current case study;\nEach case study has two rounds of presentation: an early stage presentation on exploratory work and a final presentation on analysis. Groups will be selected at random to present in one of these two rounds, with each group presenting at least once for each case study;\nLab sections on Friday will sometimes feature a presentation or tutorial, but for the most part these are built-in work periods where all group members can convene with access to a TA;\nAt the end of each case study, students will confidentially evaluate themselves and their group members with a survey through TEAMMATES. Only the teaching team will have access to this feedback.\n\nAs mentioned above, each case study will be graded as follows:\n\n(60%) at the end you submit a brief report describing your final analysis;\n\nsee here for the grading rubric;\n\n(30%) you will give at least one presentation, on your exploratory or final analysis;\n\nsee here for the grading rubric;\n\n(10%) based on your GitHub activity and the results of the TEAMMATES survey, each individual group member will receive a separate score reflecting their contribution to the team’s effort."
  },
  {
    "objectID": "syllabus/syllabus-policies.html#attendance",
    "href": "syllabus/syllabus-policies.html#attendance",
    "title": "Policies",
    "section": "Attendance",
    "text": "Attendance"
  },
  {
    "objectID": "syllabus/syllabus-policies.html#use-of-ai",
    "href": "syllabus/syllabus-policies.html#use-of-ai",
    "title": "Policies",
    "section": "Use of AI",
    "text": "Use of AI"
  },
  {
    "objectID": "syllabus/syllabus-policies.html#academic-honesty",
    "href": "syllabus/syllabus-policies.html#academic-honesty",
    "title": "Policies",
    "section": "Academic honesty",
    "text": "Academic honesty"
  }
]